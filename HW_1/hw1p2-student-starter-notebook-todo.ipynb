{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":127690,"databundleVersionId":15274049,"sourceType":"competition"},{"sourceId":14541356,"sourceType":"datasetVersion","datasetId":9287689},{"sourceId":14609165,"sourceType":"datasetVersion","datasetId":9331574},{"sourceId":14704522,"sourceType":"datasetVersion","datasetId":9394099}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HW1: Frame-Level Speech Recognition","metadata":{"id":"F9ERgBpbcMmB"}},{"cell_type":"markdown","source":"In this homework, you will be working with MFCC data consisting of 28 features at each time step/frame. Your model should be able to recognize the phoneme occured in that frame.","metadata":{"id":"CLkH6GMGcWcE"}},{"cell_type":"markdown","source":"# Schedule:\n- Checkpoint Submission (DUE 23 January 2026 @ 11:59PM EST)\n- Final Submission (DUE 6 February 2026 @ 11:59PM EST | Slack Deadline is 13 February 2026 @ 11:59PM EST)\n- Code Submission (DUE 8 February 2026 @ 11:59PM EST OR Day-of Slack submission)\n","metadata":{"id":"ezY4enYT7evo"}},{"cell_type":"markdown","source":"## Requirement Acknowledgement\nSetting the below flag to True indicates full understanding and acceptance of the following:\n1. Slack days may ONLY be used on P2 FINAL (not checkpoint) submission. I.e. you may use slack days to submit final P2 kaggle scores (such as this one) later on the **SLACK KAGGLE COMPETITION** at the expense of your Slack days.\n2. The final autolab **code submission is due 48 hours after** the conclusion of the Kaggle Deadline (or, the same day as your final kaggle submission).\n3. We will require your kaggle username here, and then we will pull your official PRIVATE kaggle leaderboard score. This submission may result in slight variance in scores/code, but we will check for acceptable discrepancies. Any discrepancies related to modifying the submission code (at the bottom of the notebook) will result in an AIV.\n4. You are NOT allowed to use any code that will pre-load models (such as those from Hugging Face, etc.).\n   You MAY use models described by papers or articles, but you MUST implement them yourself through fundamental PyTorch operations (i.e. Linear, Conv2d, etc.).\n5. You are NOT allowed to use any external data/datasets at ANY point of this assignment.\n6. You may work with teammates to run ablations/experiments, BUT you must submit your OWN code and your OWN results.\n7. Failure to comply with the prior rules will be considered an Academic Integrity Violation (AIV).\n8. Late submissions MUST be submitted through the Slack Kaggle (see writeup for details). Any submissions made to the regular Kaggle after the original deadline will NOT be considered, no matter how many slack days remain for the student.","metadata":{"id":"3-6ndg_m7evp"}},{"cell_type":"code","source":"ACKNOWLEDGED = True #TODO: Only set Acknowledged to True if you have read the above acknowlegements and agree to ALL of them.","metadata":{"id":"Dcb6jKbr7evq","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:03:53.897404Z","iopub.execute_input":"2026-02-02T07:03:53.898178Z","iopub.status.idle":"2026-02-02T07:03:53.904771Z","shell.execute_reply.started":"2026-02-02T07:03:53.898147Z","shell.execute_reply":"2026-02-02T07:03:53.904009Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Dataset Description\n\nLet's start by understanding the dataset for this homework.\n\nOur data consists of 3 folders (train-clean-100, dev-clean and test-clean). The training and validation datasets (train-clean-100 and dev-clean) each contain 2 subfolders (mfcc and transcript). The 'mfcc' subfolder contains mel spectrograms (explained below and in writeup), while the 'transcript' subfolder contains their corresponding transcripts. However, the test dataset (test-clean) contains only the 'mfcc' subfolder without the corresponding transcripts, which will later be predicted by your model.\n\n\n## 1. Audio Representation.\nThe 'mfcc' subfolders contain many `*.npy` files of mel spectrograms. .npy files are used to store numpy arrays.\n\nEach .npy file represents a short speech recording. For example, one recording might be someone saying, \"This is the age of AI.\" This recording is converted into a mel spectrogram, which is used to represent all forms of audio signals in a computer. Such representation is important in signal and speech processing tasks, especially in machine learning.\n\nCompared to raw audio, mel spectrograms are better for speech processing because they capture both the timing and the frequencies of the sound. At each moment in time, they show which frequencies are present in the sound. This makes it easier for computers to understand and process speech.\n\nWhen converting raw audio to spectrograms, you do not process the whole audio at once. Instead, you process small frames at a time as you stride over the entire audio length. This means that if you have an audio file of 100 seconds, you may decide to process 10 seconds at a time, striding by one second. In this case, the frame size is 10 seconds. The frame size and the number of timesteps (seconds, milliseconds, etc.) depend on individual choice.\n\nWhen processing each frame, you extract a number of features that represent that frame's audio. For instance, in the audio recording of \"This is the age of AI,\" the frame corresponding to \"AI\" will have features that represent how \"AI\" is pronounced, the vocal tract, and the effect of the environment in which it was recorded. For clarity, when we say features, you should think of columns. One feature/column may have information about the gender of the person who made the speech. Another may have information about the age of the person. Another may have information about the environment where the speech was recorded. Basically, the main properties that make up a speech are encoded in those features, which combine in some way to make the audio.\n\nSince we want to recognize the word as it was pronounced despite the environment and other variabilities, we usually normalize to eliminate or minimize such effects.\n\nOur spectrograms contain 28 features. Essentially, the number of features may be different. They may depend on how the raw audio data was converted into mel spectrograms.\n\n## 2. Transcripts\nRemember where we mentioned frames? Well, in our dataset, audio frames have corresponding target transcripts. For instance the abbreviation \"AI\", in our example above, if present in the recordings, will have transcripts: /eɪ aɪ/. This means that you will have two frames one for  /eɪ/ and another for /aɪ/.\n\nThis way of representing pronounciation in text form is called ***phonetic transcription***, \"the conversion of spoken words the way they are pronounced instead of how they are written\"[[link]](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://krisp.ai/blog/phonetic-transcription/%23:~:text%3Dphonetic%2520transcriptions%2520done.-,What%2520are%2520Phonetic%2520Transcriptions%253F,verbatim%2520to%2520intelligent%2520verbatim%2520transcriptions.&ved=2ahUKEwiV6LO6hrSHAxUKSvEDHcvwAAsQFnoECB0QAw&usg=AOvVaw0VqoWceOzdVwe-AvdyyWqJ). In this case letters 'A' and 'I' are pronounce /eɪ/ and /aɪ/, respectively. Both letters in different words may be pronounced differently.\n\nThe produced representation of the speech is referred to as phonemes. Various .npy files that contain recordings of the sentence **\"This is the age of AI.\"** would map to **\"/ðɪs ɪz ðə eɪdʒ əv eɪ aɪ/.\"** The phonemes representation for **Chelsea sucks** would be **/ˈtʃɛl.si sʌks/**\n\nGoing inside the .npy files. Each .npy file contains vectors which have 28 features/dimensions/columns. The number of vectors in the file corresponds to the number of frames in the recording. And each single frame has a corresponding phoneme in the transcript.\n\nFor instance the .npy file for \"This is the age of AI\" --> \"/ðɪs ɪz ðə eɪdʒ əv eɪ aɪ/\" might have 13 frames (13 vectors):\n\n- /ðɪs/ has 3 phonemes: /ð/, /ɪ/, /s/  \n- /ɪz/ has 2 phonemes: /ɪ/, /z/\n- /ðə/ has 2 phonemes: /ð/, /ə/\n- /eɪdʒ/ has 2 phonemes: /eɪ/, /dʒ/\n- /əv/ has 2 phonemes: /ə/, /v/\n- /eɪ aɪ/ has 2 phonemes: /eɪ/, /aɪ/\n\n**Chelsea sucks** --> **/ˈtʃɛl.si sʌks/** might have 8 frames (8 vectors):\n\n- /ˈtʃɛl.si/ has 4 phonemes: /tʃ/, /ɛ/, /l/, /si/\n- /sʌks/ has 4 phonemes: /s/, /ʌ/, /k/, /s/\n\nNote that recordings of different sentences may have different number of frames.\n\nThe model you will produce must take a vector of a particular frame and predict the frame's transcript as accurately as possible.\n\nTherefore, the **__getitem__** method of your dataset class must return a 28 dimensional vector of a particular frame and its corresponding phoneme transcript.\n\nThis means that, while you are doing your data preprocessing in the **__init__** method, you need stack all vectors from all recordings on top of each other. You must do this for all transcripts as well and remember to ensure the correspondance between frames and their phoneme mapping is maintained.\n\nFor our dataset of two samples above, if you stack the recordings together, you get:\n\n\n| Frame | Feature 1 | Feature 2 | ... | Feature 28 | Phoneme |\n|-------|-----------|-----------|-----|------------|---------|\n| 0     | v0_1      | v0_2      | ... | v0_28      | /ð/     |\n| 1     | v1_1      | v1_2      | ... | v1_28      | /ɪ/     |\n| 2     | v2_1      | v2_2      | ... | v2_28      | /s/     |\n| 3     | v3_1      | v3_2      | ... | v3_28      | /ɪ/     |\n| 4     | v4_1      | v4_2      | ... | v4_28      | /z/     |\n| 5     | v5_1      | v5_2      | ... | v5_28      | /ð/     |\n| 6     | v6_1      | v6_2      | ... | v6_28      | /ə/     |\n| 7     | v7_1      | v7_2      | ... | v7_28      | /eɪ/    |\n| 8     | v8_1      | v8_2      | ... | v8_28      | /dʒ/    |\n| 9     | v9_1      | v9_2      | ... | v9_28      | /ə/     |\n| 10    | v10_1     | v10_2     | ... | v10_28     | /v/     |\n| 11    | v11_1     | v11_2     | ... | v11_28     | /eɪ/    |\n| 12    | v12_1     | v12_2     | ... | v12_28     | /aɪ/    |\n| 13    | v13_1     | v13_2     | ... | v13_28     | /tʃ/    |\n| 14    | v14_1     | v14_2     | ... | v14_28     | /ɛ/     |\n| 15    | v15_1     | v15_2     | ... | v15_28     | /l/     |\n| 16    | v16_1     | v16_2     | ... | v16_28     | /si/     |\n| 17    | v17_1     | v17_2     | ... | v17_28     | /s/     |\n| 18    | v18_1     | v18_2     | ... | v18_28     | /ʌ/     |\n| 19    | v19_1     | v19_2     | ... | v19_28     | /k/     |\n| 20    | v20_1     | v20_2     | ... | v20_28     | /s/     |\n\n\nSo, if you pass index 5 to **__getitem__**, you will get back vector v5 (v5_1, v5_2, ..., v5_28) and transcript **/ð/**. Ideally, if you have a well trained model, it should take v5 and return **/ð/**. And the call to **__len__** would return 21 which the training loop would use to go through the whole dataset.\n\n## Context\n\nIn the dataset we are using, a few millisecs were used to convert raw audio to mel spectrogram and extract the 28 features.\nSince each vector represents only a few millisecs of speech, it may not be sufficient to feed only a single vector into the network at a time. Instead, it may be useful to provide the network with some “context” of size K around each vector in terms of additional vectors from the speech input.\n\nConcretely, a context of size 3 would mean that we provide an input of size (7, 28) to the network - the size 7 can be explained as: the vector to predict the label for, 3 vectors preceding this vector, and 3 vectors following it. It is worth thinking about how you would handle providing context before one of the first K frames of an utterance or after one of the last K frames.\n\nThere are several ways to implement this, but you could try the simplest one:\n- Concatenating all utterances and padding with K 0-valued vectors before and after the resulting matrix\n\nIf you use a context of 3 on the above table, you get the following table:\n\n| Frame | Feature 1 | Feature 2 | ... | Feature 28 | Phoneme | Context Vectors |\n|-------|-----------|-----------|-----|------------|---------|----------------|\n| 0     | v0_1      | v0_2      | ... | v0_28      | /ð/     | [0, 0, 0, ..., 0] (Padding), [0, 0, 0, ..., 0] (Padding), [0, 0, 0, ..., 0] (Padding), v0, v1, v2, v3 |\n| 1     | v1_1      | v1_2      | ... | v1_28      | /ɪ/     | [0, 0, 0, ..., 0] (Padding), [0, 0, 0, ..., 0] (Padding), v0, v1, v2, v3, v4 |\n| 2     | v2_1      | v2_2      | ... | v2_28      | /s/     | [0, 0, 0, ..., 0] (Padding), v0, v1, v2, v3, v4, v5 |\n| 3     | v3_1      | v3_2      | ... | v3_28      | /ɪ/     | v0, v1, v2, v3, v4, v5, v6 |\n| 4     | v4_1      | v4_2      | ... | v4_28      | /z/     | v1, v2, v3, v4, v5, v6, v7 |\n| 5     | v5_1      | v5_2      | ... | v5_28      | /ð/     | v2, v3, v4, v5, v6, v7, v8 |\n| 6     | v6_1      | v6_2      | ... | v6_28      | /ə/     | v3, v4, v5, v6, v7, v8, v9 |\n| 7     | v7_1      | v7_2      | ... | v7_28      | /eɪ/    | v4, v5, v6, v7, v8, v9, v10 |\n| 8     | v8_1      | v8_2      | ... | v8_28      | /dʒ/    | v5, v6, v7, v8, v9, v10, v11 |\n| 9     | v9_1      | v9_2      | ... | v9_28      | /ə/     | v6, v7, v8, v9, v10, v11, v12 |\n| 10    | v10_1     | v10_2     | ... | v10_28     | /v/     | v7, v8, v9, v10, v11, v12, v13 |\n| 11    | v11_1     | v11_2     | ... | v11_28     | /eɪ/    | v8, v9, v10, v11, v12, v13, v14 |\n| 12    | v12_1     | v12_2     | ... | v12_28     | /aɪ/    | v9, v10, v11, v12, v13, v14, v15 |\n| 13    | v13_1     | v13_2     | ... | v13_28     | /tʃ/    | v10, v11, v12, v13, v14, v15, v16 |\n| 14    | v14_1     | v14_2     | ... | v14_28     | /ɛ/     | v11, v12, v13, v14, v15, v16, v17 |\n| 15    | v15_1     | v15_2     | ... | v15_28     | /l/     | v12, v13, v14, v15, v16, v17, v18 |\n| 16    | v16_1     | v16_2     | ... | v16_28     | /s/     | v13, v14, v15, v16, v17, v18, v19 |\n| 17    | v17_1     | v17_2     | ... | v17_28     | /i/     | v14, v15, v16, v17, v18, v19, v20 |\n| 18    | v18_1     | v18_2     | ... | v18_28     | /s/     | v15, v16, v17, v18, v19, v20, v21 |\n| 19    | v19_1     | v19_2     | ... | v19_28     | /ʌ/     | v16, v17, v18, v19, v20, v21, [0, 0, 0, ..., 0] (Padding) |\n| 20    | v20_1     | v20_2     | ... | v20_28     | /k/     | v17, v18, v19, v20, v21, [0, 0, 0, ..., 0] (Padding), [0, 0, 0, ..., 0] (Padding) |\n| 21    | v21_1     | v21_2     | ... | v21_28     | /s/     | v18, v19, v20, v21, [0, 0, 0, ..., 0] (Padding), [0, 0, 0, ..., 0] (Padding), [0, 0, 0, ..., 0] (Padding) |\n\n\nNow, if you want to predict the output of vector v5, you won't just pass vector v5 alone. You will concatenate 3 vectors before it and 3 vectors after, which makes it 7 vectors ([v2, v3, v4, v5, v6, v7, v8 ]) . This needs to be reflected in your **__getitem__** method. Meaning it should return an array of shape (7, 28), in this example.\n\nHence your model is going to be taking a tensor (array) of shape (7, 28) in this example.","metadata":{"id":"P2vp3N7qr_5V"}},{"cell_type":"markdown","source":"# Libraries","metadata":{"id":"z4vZbDmJvMp1"}},{"cell_type":"code","source":"!pip install wandb==0.24.0 --quiet","metadata":{"id":"rwYu9sSUnSho","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:04:00.051120Z","iopub.execute_input":"2026-02-02T07:04:00.051413Z","iopub.status.idle":"2026-02-02T07:04:46.065285Z","shell.execute_reply.started":"2026-02-02T07:04:00.051386Z","shell.execute_reply":"2026-02-02T07:04:46.064386Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'wandb' candidate (version 0.24.0 at https://files.pythonhosted.org/packages/22/a1/8d68a914c030e897c306c876d47c73aa5d9ca72be608971290d3a5749570/wandb-0.24.0-py3-none-manylinux_2_28_x86_64.whl (from https://pypi.org/simple/wandb/) (requires-python:>=3.8))\nReason for being yanked: <none given>\u001b[0m\u001b[33m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.8/22.8 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install torchaudio --quiet","metadata":{"id":"ijYNIOkpYrXf","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:04:46.067007Z","iopub.execute_input":"2026-02-02T07:04:46.067264Z","iopub.status.idle":"2026-02-02T07:04:49.233785Z","shell.execute_reply.started":"2026-02-02T07:04:46.067234Z","shell.execute_reply":"2026-02-02T07:04:49.232938Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport sklearn\nimport gc\nimport zipfile\nimport bisect\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport os\nimport json\nimport datetime\nimport wandb\nimport yaml\nimport torchaudio.transforms as tat\nimport torchaudio\n\n## New\nfrom torch.utils.data import Subset, ConcatDataset\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Device: \", device)","metadata":{"id":"qI4qfx7tiBZt","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:04:49.235182Z","iopub.execute_input":"2026-02-02T07:04:49.235508Z","iopub.status.idle":"2026-02-02T07:04:59.970432Z","shell.execute_reply.started":"2026-02-02T07:04:49.235461Z","shell.execute_reply":"2026-02-02T07:04:59.969788Z"}},"outputs":[{"name":"stdout","text":"Device:  cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Set random seed for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:04:59.972144Z","iopub.execute_input":"2026-02-02T07:04:59.972673Z","iopub.status.idle":"2026-02-02T07:04:59.984853Z","shell.execute_reply.started":"2026-02-02T07:04:59.972649Z","shell.execute_reply":"2026-02-02T07:04:59.984211Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Mount Google Drive (Colab Only)","metadata":{"id":"eqsFLqa6rCuc"}},{"cell_type":"code","source":"''' If you are using colab, you can import google drive to save model checkpoints in a folder\n    If you are NOT running on Colab, skip this cell\n'''\n#import sys\n#if \"google.colab\" in sys.modules:\n#    from google.colab import drive\n#    drive.mount('/content/drive')","metadata":{"id":"Z23Nag1jq_yA","trusted":true,"execution":{"iopub.status.busy":"2026-02-01T11:21:56.049468Z","iopub.execute_input":"2026-02-01T11:21:56.049706Z","iopub.status.idle":"2026-02-01T11:21:56.063085Z","shell.execute_reply.started":"2026-02-01T11:21:56.049675Z","shell.execute_reply":"2026-02-01T11:21:56.062454Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"' If you are using colab, you can import google drive to save model checkpoints in a folder\\n    If you are NOT running on Colab, skip this cell\\n'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"### PHONEME LIST\nPHONEMES = [\n            '[SIL]',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',\n            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n            'V',     'W',     'Y',     'Z',     'ZH',    '[SOS]', '[EOS]']","metadata":{"id":"N-9qE20hmCgQ","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:05:28.624912Z","iopub.execute_input":"2026-02-02T07:05:28.625546Z","iopub.status.idle":"2026-02-02T07:05:28.629932Z","shell.execute_reply.started":"2026-02-02T07:05:28.625505Z","shell.execute_reply":"2026-02-02T07:05:28.629129Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Kaggle API Setup (Colab/AWS Only)","metadata":{"id":"ZIi0Big7vPa9"}},{"cell_type":"markdown","source":"This section contains code that helps you install kaggle's API, creating kaggle.json with you username and API key details. Make sure to input those in the given code to ensure you can download data from the competition successfully.","metadata":{"id":"BBCbeRhixGM7"}},{"cell_type":"code","source":"''' If you are running on Kaggle, skip this cell\n'''\n\n#if \"KAGGLE_KERNEL_RUN_TYPE\" not in os.environ:\n#    !pip install --upgrade kaggle\n#    !mkdir /root/.kaggle\n#\n#    with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n#        # TODO: Put your kaggle username & key here\n#        f.write('{\"username\":\"YOUR_USERNAME\",\"key\":\"YOUR_KEY\"}')\n#\n#    !chmod 600 /root/.kaggle/kaggle.json","metadata":{"id":"TPBUd7Cnl-Rx","trusted":true,"execution":{"iopub.status.busy":"2026-02-01T11:21:56.076465Z","iopub.execute_input":"2026-02-01T11:21:56.077024Z","iopub.status.idle":"2026-02-01T11:21:56.088299Z","shell.execute_reply.started":"2026-02-01T11:21:56.077001Z","shell.execute_reply":"2026-02-01T11:21:56.087470Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"' If you are running on Kaggle, skip this cell\\n'"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# Kaggle Credentials & Client Setup (Kaggle Only)","metadata":{"id":"YTs0uaLU7evw"}},{"cell_type":"code","source":"''' If you NOT running on Kaggle, skip this cell\n'''\n\nif \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ:\n  # IMPORTANT: Set kaggle Constants & environment variables for API Usage\n  \"\"\"Mine\"\"\"\n  with open(\"/kaggle/input/mnljson/kaggle.json\", \"r\") as f:\n    creds = json.load(f)\n      \n  #KAGGLE_USERNAME = \"Your Kaggle Username\"\n  #KAGGLE_API_KEY = \"Your Kaggle API Key\"\n  KAGGLE_USERNAME = creds[\"username\"]\n  KAGGLE_API_KEY = creds[\"key\"]\n\n\n  # Set in env for kaggle client\n  import os\n  os.environ[\"KAGGLE_USERNAME\"] = KAGGLE_USERNAME\n  os.environ[\"KAGGLE_KEY\"] = KAGGLE_API_KEY\n  print(\"Kaggle credentials set.\")\n","metadata":{"id":"0WbCmoGI7evw","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:04:59.985632Z","iopub.execute_input":"2026-02-02T07:04:59.985917Z","iopub.status.idle":"2026-02-02T07:05:00.006492Z","shell.execute_reply.started":"2026-02-02T07:04:59.985885Z","shell.execute_reply":"2026-02-02T07:05:00.006000Z"}},"outputs":[{"name":"stdout","text":"Kaggle credentials set.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Download Dataset (Colab/AWS Only)","metadata":{"id":"npqu-ZG47evw"}},{"cell_type":"markdown","source":"If working on Kaggle, please add the competition as an input (go to add input, filter by Your Work and Competitions, and add the S26 HW1P2 Competition by clicking the + button). The competition already has the dataset, so you can use that as your \"ROOT\" directory! For other platforms, you will have to download the data and mount it locally","metadata":{"id":"CVOSKn0AdltM"}},{"cell_type":"code","source":"#!pip install kaggle==1.6.14 kagglesdk==0.1.13\n\n''' If running on Kaggle, skip this cell\n'''\n\n#if \"KAGGLE_KERNEL_RUN_TYPE\" not in os.environ:\n    # commands to download data from kaggle\n#    !kaggle competitions download -c hw-1-p-2-spring-2026-student-competition\n\n    # Unzip downloaded data\n#    !unzip -qo /content/hw-1-p-2-spring-2026-student-competition.zip -d '/content/data'","metadata":{"id":"if2Somqfbje1","trusted":true,"execution":{"iopub.status.busy":"2026-02-01T11:21:56.139477Z","iopub.execute_input":"2026-02-01T11:21:56.139984Z","iopub.status.idle":"2026-02-01T11:21:56.144715Z","shell.execute_reply.started":"2026-02-01T11:21:56.139948Z","shell.execute_reply":"2026-02-01T11:21:56.143805Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"' If running on Kaggle, skip this cell\\n'"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# Parameters Configuration","metadata":{"id":"qNacQ8bpt9nw"}},{"cell_type":"markdown","source":"Storing your parameters and hyperparameters in a single configuration dictionary makes it easier to keep track of them during each experiment. It can also be used with weights and biases to log your parameters for each experiment and keep track of them across multiple experiments.","metadata":{"id":"WE7tsinAuLNy"}},{"cell_type":"code","source":"config = {\n    'Name': 'Moan Lobago', # Write your name here\n    'subset': 1.0, # Subset of train/val dataset to use (1.0 == 100% of data)\n    'context': 32,\n    'archetype': 'diamond', # Default Values: pyramid, diamond, inverse-pyramid,cylinder\n    'activations': 'GELU',\n    'learning_rate': 5e-4,\n    'dropout': 0.15,\n    'optimizers': 'AdamW',\n    'scheduler': 'CosineAnnealingLR',\n    'epochs': 25,\n    'batch_size': 2048,\n    'weight_decay': 0.05,\n    'weight_initialization': 'kaiming_normal', # e.g kaiming_normal, kaiming_uniform, uniform, xavier_normal or xavier_uniform\n    'augmentations': 'Both', # Options: [\"FreqMask\", \"TimeMask\", \"Both\", null]\n    'freq_mask_param': 4,\n    'time_mask_param': 8\n }","metadata":{"id":"S5gMTwnSnp8K","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:05:00.007153Z","iopub.execute_input":"2026-02-02T07:05:00.007390Z","iopub.status.idle":"2026-02-02T07:05:00.011333Z","shell.execute_reply.started":"2026-02-02T07:05:00.007357Z","shell.execute_reply":"2026-02-02T07:05:00.010674Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"config","metadata":{"id":"vzeqgWS9pumb","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:05:00.012831Z","iopub.execute_input":"2026-02-02T07:05:00.013082Z","iopub.status.idle":"2026-02-02T07:05:00.033492Z","shell.execute_reply.started":"2026-02-02T07:05:00.013062Z","shell.execute_reply":"2026-02-02T07:05:00.032869Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'Name': 'Moan Lobago',\n 'subset': 1.0,\n 'context': 32,\n 'archetype': 'diamond',\n 'activations': 'GELU',\n 'learning_rate': 0.0005,\n 'dropout': 0.15,\n 'optimizers': 'AdamW',\n 'scheduler': 'CosineAnnealingLR',\n 'epochs': 25,\n 'batch_size': 2048,\n 'weight_decay': 0.05,\n 'weight_initialization': 'kaiming_normal',\n 'augmentations': 'Both',\n 'freq_mask_param': 4,\n 'time_mask_param': 8}"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# Dataset Class","metadata":{"id":"FYeyFHQ1yRi4"}},{"cell_type":"markdown","source":"This section covers the dataset/dataloader class for speech data. You will have to spend time writing code to create this class successfully. We have given you a lot of comments guiding you on what code to write at each stage, from top to bottom of the class. Please try and take your time figuring this out, as it will immensely help in creating dataset/dataloader classes for future homeworks.\n\nBefore running the following cells, please take some time to analyse the structure of data. Try loading a single MFCC and its transcipt, print out the shapes and print out the values. Do the transcripts look like phonemes?","metadata":{"id":"2_7QgMbBdgPp"}},{"cell_type":"code","source":"# Dataset class to load train and validation data\n\nclass AudioDataset(torch.utils.data.Dataset):\n\n    def __init__(self, root, phonemes = PHONEMES, context=0, partition= \"train-clean-100\"): # Feel free to add more arguments\n\n        self.context    = context\n        self.phonemes   = phonemes\n        self.subset = config['subset']\n\n        # TODO: Initialize augmentations. Read the Pytorch torchaudio documentations on timemasking and frequencymasking\n        self.freq_masking = tat.FrequencyMasking(config['freq_mask_param'])\n        self.time_masking = tat.TimeMasking(config['time_mask_param'])\n\n\n        # TODO: MFCC directory - use partition to acces train/dev directories from kaggle data using root\n        self.mfcc_dir       = os.path.join(root, partition, \"mfcc\")\n        # TODO: Transcripts directory - use partition to acces train/dev directories from kaggle data using root\n        self.transcript_dir = os.path.join(root, partition, \"transcript\")\n\n        # TODO: List files in sefl.mfcc_dir using os.listdir in SORTED order\n        mfcc_names          = sorted(os.listdir(self.mfcc_dir))\n        # TODO: List files in self.transcript_dir using os.listdir in SORTED order\n        transcript_names    = sorted(os.listdir(self.transcript_dir))\n\n        # Compute size of data subset\n        subset_size = int(self.subset * len(mfcc_names))\n\n        # Select subset of data to use\n        mfcc_names = mfcc_names[:subset_size]\n        transcript_names = transcript_names[:subset_size]\n\n        # Making sure that we have the same no. of mfcc and transcripts\n        assert len(mfcc_names) == len(transcript_names)\n\n        self.mfccs, self.transcripts = [], []\n\n\n        # TODO: Iterate through mfccs and transcripts\n        for i in tqdm(range(len(mfcc_names))):\n\n            # TODO: Load a single mfcc. Hint: Use numpy\n            mfcc_path = os.path.join(self.mfcc_dir, mfcc_names[i])\n            mfcc             = np.load(mfcc_path)\n            # TODO: Do Cepstral Normalization of mfcc along the Time Dimension (Think about the correct axis)\n            mfccs_normalized = (mfcc -mfcc.mean(axis=0, keepdims=True)) / (mfcc.std(axis=0, keepdims=True) +1e-8)\n\n            # Convert mfcc to tensor\n            mfccs_normalized = torch.tensor(mfccs_normalized, dtype=torch.float32)\n\n            # TODO: Load the corresponding transcript\n            # Remove [SOS] and [EOS] from the transcript\n            # (Is there an efficient way to do this without traversing through the transcript?)\n            # Note that SOS will always be in the starting and EOS at end, as the name suggests.\n            transcript_path = os.path.join(self.transcript_dir, transcript_names[i])\n            transcript_data = np.load(transcript_path, allow_pickle=True)\n            transcript  = transcript_data[1:-1]\n\n            # The available phonemes in the transcript are of string data type\n            # But the neural network cannot predict strings as such.\n            # Hence, we map these phonemes to integers\n\n            # TODO: Map the phonemes to their corresponding list indexes in self.phonemes\n            phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(self.phonemes)}\n            transcript_indices = [phoneme_to_index[ph] for ph in transcript]\n            # Now, if an element in the transcript is 0, it means that it is 'SIL' (as per the above example)\n\n            # Convert transcript to tensor\n            transcript_indices = torch.tensor(transcript_indices, dtype=torch.int64)\n\n            # Append each mfcc to self.mfcc, transcript to self.transcript\n            self.mfccs.append(mfccs_normalized)\n            self.transcripts.append(transcript_indices)\n\n        # NOTE:\n        # Each mfcc is of shape T1 x 28, T2 x 28, ...\n        # Each transcript is of shape (T1+2), (T2+2) before removing [SOS] and [EOS]\n\n        # TODO: Concatenate all mfccs in self.mfccs such that\n        # the final shape is T x 28 (Where T = T1 + T2 + ...)\n        # Hint: Use torch to concatenate\n        self.mfccs          = torch.cat(self.mfccs, dim=0)\n\n        # TODO: Concatenate all transcripts in self.transcripts such that\n        # the final shape is (T,) meaning, each time step has one phoneme output\n        # Hint: Use torch to concatenate\n        self.transcripts    = torch.cat(self.transcripts, dim=0)\n\n        # Length of the dataset is now the length of concatenated mfccs/transcripts\n        self.length = len(self.mfccs)\n\n        # Take some time to think about what we have done.\n        # self.mfcc is an array of the format (Frames x Features).\n        # Our goal is to recognize phonemes of each frame\n\n        # We can introduce context by padding zeros on top and bottom of self.mfcc\n        # Hint: Use torch.nn.functional.pad\n        # torch.nn.functional.pad takes the padding in the form of (left, right, top, bottom) for 2D data\n        if self.context > 0:\n            self.mfccs = torch.nn.functional.pad(self.mfccs, (0, 0, self.context, self.context), mode='constant', value=0) # TODO\n\n\n    def __len__(self):\n        return self.length\n\n    def collate_fn(self, batch):\n      x, y = zip(*batch)\n      x = torch.stack(x, dim=0)\n\n      # Apply augmentations with 70% probability (You can modify the probability)\n      if np.random.rand() < 0.70:\n        x = x.transpose(1, 2)  # Shape: (batch_size, freq, time)\n        x = self.freq_masking(x)\n        x = self.time_masking(x)\n        x = x.transpose(1, 2)  # Shape back to: (batch_size, time, freq)\n\n      return x, torch.tensor(y)\n\n    def __getitem__(self, ind):\n        # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n        start_idx = ind\n        end_idx = ind + 2 * self.context +1 \n        frames = self.mfccs[start_idx:end_idx]\n\n        # After slicing, you get an array of shape 2*context+1 x 28.\n\n        phonemes = self.transcripts[ind]\n\n        return frames, phonemes\n","metadata":{"id":"HYU4NAH65dSb","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:05:34.473090Z","iopub.execute_input":"2026-02-02T07:05:34.473387Z","iopub.status.idle":"2026-02-02T07:05:34.486897Z","shell.execute_reply.started":"2026-02-02T07:05:34.473361Z","shell.execute_reply":"2026-02-02T07:05:34.486089Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"'''\n# Dataset class to load test data\n\nclass AudioTestDataset(torch.utils.data.Dataset):\n\n    # TODO: Create a test dataset class similar to the previous class but you dont have transcripts for this\n    # Imp: Read the mfccs in sorted order, do NOT shuffle the data here or in your dataloader.\n\n    # IMPORTANT: Load complete test data to use, DO NOT select subset of test data, else you will get errors when submitting on Kaggle.\n    def __init__(self, root, context=0, partition=\"test-clean\"):\n        self.context = context\n        self.mfcc_dir = os.path.join(root, partition, \"mfcc\")\n        \n        # List and sort mfcc files\n        mfcc_names = sorted(os.listdir(self.mfcc_dir))\n        \n        self.mfccs = []\n        self.filenames = []\n        \n        for mfcc_name in tqdm(mfcc_names):\n            mfcc_path = os.path.join(self.mfcc_dir, mfcc_name)\n            mfcc = np.load(mfcc_path)\n            \n            # Cepstral normalization\n            mfcc_normalized = (mfcc - mfcc.mean(axis=0, keepdims=True)) / (mfcc.std(axis=0, keepdims=True) + 1e-8)\n            mfcc_normalized = torch.tensor(mfcc_normalized, dtype=torch.float32)\n            \n            self.mfccs.append(mfcc_normalized)\n            self.filenames.append(mfcc_name)\n        \n        # Store mfccs individually (not concatenated) for test data\n        if self.context > 0:\n            self.mfccs = [torch.nn.functional.pad(mfcc, (0, 0, self.context, self.context), \n                                                 mode='constant', value=0) \n                         for mfcc in self.mfccs]\n    \n    def __len__(self):\n        return len(self.mfccs)\n    \n    def __getitem__(self, ind):\n        frames = self.mfccs[ind : ind + 2 * self.context +1]\n        \n        return frames\n\n'''","metadata":{"id":"C0rme6iT5dSb","trusted":true,"execution":{"iopub.status.busy":"2026-02-01T11:21:56.185674Z","iopub.execute_input":"2026-02-01T11:21:56.186055Z","iopub.status.idle":"2026-02-01T11:21:56.203754Z","shell.execute_reply.started":"2026-02-01T11:21:56.186034Z","shell.execute_reply":"2026-02-01T11:21:56.203196Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'\\n# Dataset class to load test data\\n\\nclass AudioTestDataset(torch.utils.data.Dataset):\\n\\n    # TODO: Create a test dataset class similar to the previous class but you dont have transcripts for this\\n    # Imp: Read the mfccs in sorted order, do NOT shuffle the data here or in your dataloader.\\n\\n    # IMPORTANT: Load complete test data to use, DO NOT select subset of test data, else you will get errors when submitting on Kaggle.\\n    def __init__(self, root, context=0, partition=\"test-clean\"):\\n        self.context = context\\n        self.mfcc_dir = os.path.join(root, partition, \"mfcc\")\\n        \\n        # List and sort mfcc files\\n        mfcc_names = sorted(os.listdir(self.mfcc_dir))\\n        \\n        self.mfccs = []\\n        self.filenames = []\\n        \\n        for mfcc_name in tqdm(mfcc_names):\\n            mfcc_path = os.path.join(self.mfcc_dir, mfcc_name)\\n            mfcc = np.load(mfcc_path)\\n            \\n            # Cepstral normalization\\n            mfcc_normalized = (mfcc - mfcc.mean(axis=0, keepdims=True)) / (mfcc.std(axis=0, keepdims=True) + 1e-8)\\n            mfcc_normalized = torch.tensor(mfcc_normalized, dtype=torch.float32)\\n            \\n            self.mfccs.append(mfcc_normalized)\\n            self.filenames.append(mfcc_name)\\n        \\n        # Store mfccs individually (not concatenated) for test data\\n        if self.context > 0:\\n            self.mfccs = [torch.nn.functional.pad(mfcc, (0, 0, self.context, self.context), \\n                                                 mode=\\'constant\\', value=0) \\n                         for mfcc in self.mfccs]\\n    \\n    def __len__(self):\\n        return len(self.mfccs)\\n    \\n    def __getitem__(self, ind):\\n        frames = self.mfccs[ind : ind + 2 * self.context +1]\\n        \\n        return frames\\n\\n'"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"class AudioTestDataset(torch.utils.data.Dataset):\n    def __init__(self, root, context=20, partition=\"test-clean\"):\n        self.context = context\n        self.mfcc_dir = os.path.join(root, partition, \"mfcc\")\n\n        mfcc_names = sorted(os.listdir(self.mfcc_dir))\n        self.mfccs = []\n\n        for mfcc_name in tqdm(mfcc_names):\n            mfcc_path = os.path.join(self.mfcc_dir, mfcc_name)\n            mfcc = np.load(mfcc_path)\n\n            mfcc_normalized = (mfcc - mfcc.mean(axis=0, keepdims=True)) / \\\n                              (mfcc.std(axis=0, keepdims=True) + 1e-8)\n            mfcc_normalized = torch.tensor(mfcc_normalized, dtype=torch.float32)\n\n            # PAD EACH UTTERANCE IN TIME DIMENSION\n            if self.context > 0:\n                mfcc_normalized = torch.nn.functional.pad(\n                    mfcc_normalized,\n                    (0, 0, self.context, self.context),\n                    mode=\"constant\",\n                    value=0\n                )\n\n            self.mfccs.append(mfcc_normalized)\n\n    def __len__(self):\n        return len(self.mfccs)\n\n    def __getitem__(self, ind):\n        \"\"\"\n        Returns:\n          frames: Tensor of shape (T, 41, 28)\n        \"\"\"\n        mfcc = self.mfccs[ind]   # (T + 2C, 28)\n        T = mfcc.shape[0] - 2 * self.context\n\n        frames = []\n\n        for t in range(T):\n            start = t\n            end   = t + 2 * self.context + 1\n            frames.append(mfcc[start:end])   # (41, 28)\n\n        frames = torch.stack(frames, dim=0)   # (T, 41, 28)\n        return frames\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:05:40.096672Z","iopub.execute_input":"2026-02-02T07:05:40.097214Z","iopub.status.idle":"2026-02-02T07:05:40.105211Z","shell.execute_reply.started":"2026-02-02T07:05:40.097173Z","shell.execute_reply":"2026-02-02T07:05:40.104626Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Create Datasets","metadata":{"id":"2mlwaKlDt_2c"}},{"cell_type":"markdown","source":"Notes:\n\n- You will need to set the root path which depends on your setup. For eg. if you are following out setup instruction:\n  - `Colab:` `\"/content/data/archive/11785-spring-2026-hw1p2\"`\n  - `Kaggle:` `\"/kaggle/input/hw-1-p-2-spring-2026-student-competition/archive/11785-spring-2026-hw1p2\"`\n","metadata":{"id":"YAq5svwwYvHM"}},{"cell_type":"code","source":"ROOT = \"/kaggle/input/hw-1-p-2-spring-2026-student-competition/archive/11785-spring-2026-hw1p2\" # Define the root directory of the dataset here\n\n# TODO: Create a dataset object using the AudioDataset class for the training data\ntrain_data = AudioDataset(\n    root=ROOT,\n    phonemes = PHONEMES,\n    context=config['context'],\n    partition=\"train-clean-100\"\n)\n\n# TODO: Create a dataset object using the AudioDataset class for the validation data\nval_data = AudioDataset(\n    root=ROOT,\n    phonemes = PHONEMES,\n    context=config['context'],\n    partition=\"dev-clean\"\n)\n\n# TODO: Create a dataset object using the AudioTestDataset class for the test data\ntest_data = AudioTestDataset(\n    root=ROOT,\n    context=config['context'],\n    partition=\"test-clean\"\n)","metadata":{"id":"gJvMzHhB5dSc","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:05:43.905526Z","iopub.execute_input":"2026-02-02T07:05:43.906232Z","iopub.status.idle":"2026-02-02T07:19:41.961495Z","shell.execute_reply.started":"2026-02-02T07:05:43.906202Z","shell.execute_reply":"2026-02-02T07:19:41.960693Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/28539 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a213d152be22405d8ce58ad035068a35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2703 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39c25f8d72f74d4aa1ddfbe699b55b05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2620 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66153b71342d4552a0b404f80c81d26f"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Define dataloaders for train, val and test datasets\n# Dataloaders will yield a batch of frames and phonemes of given batch_size at every iteration\n# We shuffle train dataloader but not val & test dataloader. Why?\n\ntrain_loader = torch.utils.data.DataLoader(\n    dataset     = train_data,\n    num_workers = 4,\n    batch_size  = config['batch_size'],\n    pin_memory  = True,\n    shuffle     = True,\n    collate_fn = train_data.collate_fn\n)\n\nval_loader = torch.utils.data.DataLoader(\n    dataset     = val_data,\n    num_workers = 0,\n    batch_size  = config['batch_size'],\n    pin_memory  = True,\n    shuffle     = False\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    dataset     = test_data,\n    num_workers = 0,\n    batch_size  = config['batch_size'],\n    pin_memory  = True,\n    shuffle     = False\n)\n\n\nprint(\"Batch size     : \", config['batch_size'])\nprint(\"Context        : \", config['context'])\nprint(\"Input size     : \", (2*config['context']+1)*28)\nprint(\"Output symbols : \", len(PHONEMES))\n\nprint(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\nprint(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\nprint(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))","metadata":{"id":"4mzoYfTKu14s","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:20:56.575287Z","iopub.execute_input":"2026-02-02T07:20:56.575977Z","iopub.status.idle":"2026-02-02T07:20:56.583804Z","shell.execute_reply.started":"2026-02-02T07:20:56.575946Z","shell.execute_reply":"2026-02-02T07:20:56.583125Z"}},"outputs":[{"name":"stdout","text":"Batch size     :  2048\nContext        :  32\nInput size     :  1820\nOutput symbols :  42\nTrain dataset samples = 36091157, batches = 17623\nValidation dataset samples = 1928204, batches = 942\nTest dataset samples = 2620, batches = 2\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Testing code to check if your data loaders are working\nfor i, data in enumerate(train_loader):\n    frames, phoneme = data\n    print(frames.shape, phoneme.shape)\n\n    # Visualize sample mfcc to inspect and verify everything is correctly done, especially augmentations\n    plt.figure(figsize=(10, 6))\n    plt.imshow(frames[0].numpy().T, aspect='auto', origin='lower', cmap='viridis')\n    plt.xlabel('Time')\n    plt.ylabel('Features')\n    plt.title('Feature Representation')\n    plt.show()\n\n    break","metadata":{"id":"n-GV3UvgLSoF","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:20:59.982032Z","iopub.execute_input":"2026-02-02T07:20:59.982866Z","iopub.status.idle":"2026-02-02T07:21:06.351366Z","shell.execute_reply.started":"2026-02-02T07:20:59.982836Z","shell.execute_reply":"2026-02-02T07:21:06.350644Z"}},"outputs":[{"name":"stdout","text":"torch.Size([2048, 65, 28]) torch.Size([2048])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA0kAAAIjCAYAAADWYVDIAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVDZJREFUeJzt3XeUVeW9//HPPnV6o8ww9KaYCMaAclHsBCTGSDQSSxIw147GcmMi6xcLJhE1uYkmImoKaG7UGBPrjXixwTVq7F1RERTpbfqc/vz+8DLZI8X9HWHOMLxfa81acOZznvnu/Tx7n/2dM7PHc845AQAAAAAkSaF8FwAAAAAAXQlNEgAAAAD40CQBAAAAgA9NEgAAAAD40CQBAAAAgA9NEgAAAAD40CQBAAAAgA9NEgAAAAD40CQBAAAAgA9NEgAAkCTNnz9fnudp+fLl+S4FAPKKJgkAOsmWC9BtfVx66aW75Gs+/fTTuvLKK1VXV7dLxv88Pr0/IpGI+vbtq+nTp2vlypX5Lq/L+vvf/64rr7zyc41x9dVX67777tsp9QBAdxTJdwEAsKe56qqrNHjw4HaP7bvvvrvkaz399NOaNWuWpk+froqKil3yNT6vLfsjkUjo2Wef1fz58/XUU0/pjTfeUEFBQb7L63L+/ve/a86cOZ+rUbr66qv1zW9+U1OmTGn3+He+8x2ddNJJisfjn69IANjN0SQBQCebPHmyxowZk+8yPpfm5mYVFxfvlLH8++P0009Xz549de211+qBBx7Q1KlTd8rXCMI5p0QiocLCwk77ml1NOBxWOBzOdxkAkHf8uB0AdDEPP/ywDjnkEBUXF6u0tFTHHHOM3nzzzXaZ1157TdOnT9eQIUNUUFCgmpoafe9739PGjRvbMldeeaUuueQSSdLgwYPbfqxt+fLlWr58uTzP0/z587f6+p7ntXuX4sorr5TneXrrrbd0yimnqLKyUuPHj2/7/H/9139p9OjRKiwsVFVVlU466SStWLGiw9t/yCGHSJKWLl3a7vF33nlH3/zmN1VVVaWCggKNGTNGDzzwQLvMlh/hW7x4sc466yz16NFDZWVl+u53v6vNmze3yw4aNEhf+9rX9Mgjj2jMmDEqLCzULbfcIkmqq6vThRdeqP79+ysej2vYsGG69tprlcvl2o1x1113afTo0SotLVVZWZlGjhypG264oV0myFhb5uMXv/iFbr31Vg0dOlTxeFwHHHCAnn/++bbc9OnTNWfOHElq96OKW/ziF7/QQQcdpB49eqiwsFCjR4/WPffc064ez/PU3Nys2267re3506dPb7f/Pv07STfddJO++MUvKh6Pq7a2VjNmzNjqRzgPP/xw7bvvvnrrrbd0xBFHqKioSH379tV1110nANjd8E4SAHSy+vp6bdiwod1jPXv2lCT98Y9/1LRp0zRp0iRde+21amlp0dy5czV+/Hi9/PLLGjRokCRp4cKF+uCDD3TaaaeppqZGb775pm699Va9+eabevbZZ+V5no4//ni9++67uvPOO/WrX/2q7Wv06tVL69evN9d94oknavjw4br66qvlnJMk/exnP9Nll12mqVOn6vTTT9f69ev1m9/8RoceeqhefvnlDv2I35YL9MrKyrbH3nzzTR188MHq27evLr30UhUXF+vuu+/WlClT9Ne//lXf+MY32o1x3nnnqaKiQldeeaWWLFmiuXPn6sMPP9STTz7ZrqlYsmSJTj75ZJ111lk644wztPfee6ulpUWHHXaYVq5cqbPOOksDBgzQ008/rZkzZ2r16tW6/vrrJX0yByeffLKOOuooXXvttZKkt99+W//4xz90wQUXSFLgsba444471NjYqLPOOkue5+m6667T8ccfrw8++EDRaFRnnXWWVq1apYULF+qPf/zjVvvuhhtu0Ne//nWdeuqpSqVSuuuuu3TiiSfqoYce0jHHHCPpkzV2+umn68ADD9SZZ54pSRo6dOh25+PKK6/UrFmzNGHCBJ1zzjlt+/P555/XP/7xD0Wj0bbs5s2bdfTRR+v444/X1KlTdc899+hHP/qRRo4cqcmTJ+9o2gGga3EAgE4xb948J2mbH84519jY6CoqKtwZZ5zR7nlr1qxx5eXl7R5vaWnZavw777zTSXKLFy9ue+znP/+5k+SWLVvWLrts2TInyc2bN2+rcSS5K664ou3/V1xxhZPkTj755Ha55cuXu3A47H72s5+1e/z11193kUhkq8e3tz8effRRt379erdixQp3zz33uF69erl4PO5WrFjRlj3qqKPcyJEjXSKRaHssl8u5gw46yA0fPnyrMUePHu1SqVTb49ddd52T5O6///62xwYOHOgkuQULFrSr6yc/+YkrLi527777brvHL730UhcOh91HH33knHPuggsucGVlZS6TyWx3G4OOtWU+evTo4TZt2tSWu//++50k9+CDD7Y9NmPGDLe9l+9Pr4tUKuX23Xdfd+SRR7Z7vLi42E2bNm2r52/Zf1vWy7p161wsFnMTJ0502Wy2LXfjjTc6Se4Pf/hD22OHHXaYk+Ruv/32tseSyaSrqalxJ5xwwjbrBYCuih+3A4BONmfOHC1cuLDdh/TJOxN1dXU6+eSTtWHDhraPcDissWPH6oknnmgbw/97M4lEQhs2bNC//du/SZJeeumlXVL32Wef3e7/f/vb35TL5TR16tR29dbU1Gj48OHt6t2RCRMmqFevXurfv7+++c1vqri4WA888ID69esnSdq0aZMef/xxTZ06VY2NjW1fZ+PGjZo0aZLee++9re6Gd+aZZ7Z7h+Occ85RJBLR3//+93a5wYMHa9KkSe0e+8tf/qJDDjlElZWV7bZrwoQJymazWrx4sSSpoqJCzc3NbfO3LUHH2uJb3/pWu3fQtvzo4QcffBBoX/rXxebNm1VfX69DDjmkw2vi0UcfVSqV0oUXXqhQ6F+XDGeccYbKysr03//93+3yJSUl+va3v932/1gspgMPPDBw/QDQVfDjdgDQyQ488MBt3rjhvffekyQdeeSR23xeWVlZ2783bdqkWbNm6a677tK6deva5err63ditf/y6Tvyvffee3LOafjw4dvM+5uUHZkzZ4722msv1dfX6w9/+IMWL17c7u5q77//vpxzuuyyy3TZZZdtc4x169apb9++bf//dE0lJSXq06fPVr9r8+lt2rJdr732mnr16rXdryVJ5557ru6++25NnjxZffv21cSJEzV16lQdffTR5rG2GDBgQLv/b2mYPv37VNvz0EMP6ac//aleeeUVJZPJtsf9P2Jo8eGHH0qS9t5773aPx2IxDRkypO3zW/Tr12+rr1VZWanXXnutQ18fAPKFJgkAuogtv8j/xz/+UTU1NVt9PhL51yl76tSpevrpp3XJJZfoS1/6kkpKSpTL5XT00UdvdXOBbdneRXM2m93ucz5917dcLifP8/Twww9v845oJSUln1mH1L5pnDJlisaPH69TTjlFS5YsadsuSfrBD36w1bs+WwwbNizQ1/q0bd3JLpfL6Stf+Yp++MMfbvM5e+21lySpd+/eeuWVV/TII4/o4Ycf1sMPP6x58+bpu9/9rm677TbTWFts785y7v9+B2xH/vd//1df//rXdeihh+qmm25Snz59FI1GNW/ePN1xxx2f+fyd4fPUDwBdCU0SAHQRW355vnfv3powYcJ2c5s3b9Zjjz2mWbNm6fLLL297fMs7UX7ba4a2vEPx6TuUffqdgc+q1zmnwYMHb3Wx31HhcFizZ8/WEUccoRtvvFGXXnqphgwZIumTd6Z2tF/83nvvPR1xxBFt/29qatLq1av11a9+9TOfO3ToUDU1NQX6WrFYTMcee6yOPfZY5XI5nXvuubrlllt02WWXadiwYaaxgtrenP71r39VQUGBHnnkkXbvxM2bNy/wGJ82cOBASZ/c4GLLPEhSKpXSsmXLdup2AUBXwu8kAUAXMWnSJJWVlenqq69WOp3e6vNb7ki35bv1n/7u/KfvlCap7W8ZfboZKisrU8+ePbf6nZibbropcL3HH3+8wuGwZs2atVUtzrl2tyO3OPzww3XggQfq+uuvVyKRUO/evXX44Yfrlltu0erVq7fKb+tOfbfeemu7fTh37lxlMplAd1ibOnWqnnnmGT3yyCNbfa6urk6ZTEaSttq+UCikUaNGSVLbj7oFHctie3MaDofleV67dwOXL1+u++67b5tjfPr52zJhwgTFYjH9+te/bjfHv//971VfX992xzwA6G54JwkAuoiysjLNnTtX3/nOd/TlL39ZJ510knr16qWPPvpI//3f/62DDz5YN954o8rKynTooYfquuuuUzqdVt++ffU///M/WrZs2VZjjh49WpL0//7f/9NJJ52kaDSqY489VsXFxTr99NN1zTXX6PTTT9eYMWO0ePFivfvuu4HrHTp0qH76059q5syZWr58uaZMmaLS0lItW7ZM9957r84880z94Ac/6NC+uOSSS3TiiSdq/vz5OvvsszVnzhyNHz9eI0eO1BlnnKEhQ4Zo7dq1euaZZ/Txxx/r1Vdfbff8VCqlo446SlOnTtWSJUt00003afz48fr6178e6Gs/8MAD+trXvqbp06dr9OjRam5u1uuvv6577rlHy5cvV8+ePXX66adr06ZNOvLII9WvXz99+OGH+s1vfqMvfelL2meffUxjWWyZ0+9///uaNGmSwuGwTjrpJB1zzDH65S9/qaOPPlqnnHKK1q1bpzlz5mjYsGFb/U7Q6NGj9eijj+qXv/ylamtrNXjwYI0dO3arr9WrVy/NnDlTs2bN0tFHH62vf/3rbfvzgAMOaHeTBgDoVvJ2Xz0A2MNsub3y888/v8PcE0884SZNmuTKy8tdQUGBGzp0qJs+fbp74YUX2jIff/yx+8Y3vuEqKipceXm5O/HEE92qVau2un23c5/chrpv374uFAq1u71zS0uL+/d//3dXXl7uSktL3dSpU926deu2ewvw9evXb7Pev/71r278+PGuuLjYFRcXuxEjRrgZM2a4JUuWdHh/ZLNZN3ToUDd06NC2W2wvXbrUffe733U1NTUuGo26vn37uq997Wvunnvu2WrMRYsWuTPPPNNVVla6kpISd+qpp7qNGze2+xoDBw50xxxzzDZra2xsdDNnznTDhg1zsVjM9ezZ0x100EHuF7/4Rdutxe+55x43ceJE17t3bxeLxdyAAQPcWWed5VavXm0ea8stwH/+859vVcun5yOTybjzzz/f9erVy3me1+524L///e/d8OHDXTwedyNGjHDz5s1rmz+/d955xx166KGusLDQSWq7HfinbwG+xY033uhGjBjhotGoq66uduecc47bvHlzu8xhhx3mvvjFL25V/7Rp09zAgQO3uZ8BoKvynOO3KQEA3cP8+fN12mmn6fnnn9/mHQQBAAiC30kCAAAAAB+aJAAAAADwoUkCAAAAAB9+JwkAAAAAfHgnCQAAAAB8aJIAAAAAwKfb/zHZXC6nVatWqbS0VJ7n5bscAAAAAHninFNjY6Nqa2sVCm3//aJu3yStWrVK/fv3z3cZAAAAALqIFStWqF+/ftv9fLdvkkpLSyVJ/a78sUIFBYGe4+WMX8R46wsvZ3xHy1iPtX4XDp7Nxm0b60ozpnw4ljXlQyHrzjFOlrPNVc6Yd9a1YBQK2bbXM+7PUNg2fi4bfHvdx8WmsbFjmTLbsSjDXEn285ozrh0rLx28nnDS9pPnoYStlnDCtm/CKdv4mWAvbf/Kl9j2fbbAeB4xTm04aVxrxv1jHT9kHD/Sasnadk4kZdz3aVs+ZHvJVcg4vvn6KGtda7Z8OGV7jfNyxg2w1m/My7KUzfveuG8yxm3NGa/XduFayGSTWvTub9p6hO3p9k3Slh+xCxUU0CRth6VJcsYXS1douzALxXfvJsma392bpLD1QtfSJAU8XhFMyHgs7vZNUiR4PSHP1iQZTpmf5I3nhbD1tBO35XPm87j1YsgUV8j4o/A7+OmYbQqbriylkHH/hw0vW2HjhV/Y2ASEjBck1n1pHd98fWR9zTI2MWHjhbq5STJeY3jWa5Jd2SQZD1zPGfO7+OLa2jBL+sxfw+HGDQAAAADgQ5MEAAAAAD40SQAAAADgQ5MEAAAAAD40SQAAAADgQ5MEAAAAAD40SQAAAADgQ5MEAAAAAD40SQAAAADgQ5MEAAAAAD40SQAAAADgQ5MEAAAAAD40SQAAAADgE8l3AZ3FeZIL2BJGmjzT2FFjvv+Czab8moMrTfmmQc6Uj28KXn8oZdvWfg/Xm/Jv/6DclC9+o9CUb9g7Y8p7xu39zmFPmfL/9dqBpvzwvutM+Y8eG2jKu6gprgMmvWHKr5i1d+Dsx0faasFnMH5LrLBHiylfWWLLr3u12pQfMHqlKb/qqX7BwznT0EpWZ0358rfDpnz1M7bz5gffLDPl43W281pr3Paa4qpSpnzBqwWmfMsY21orLE6Y8okXq0x5GXZn1fG2dfzBil6mfMWLcVM+vtm2+DdNbTXlo6+W2PJNprhCX9loyr805s+m/F63nWPK93vMtvY3722br7oxycDZiw981DT2TW8dason1heb8uFm23kwW2G7Xiv8KPgFTDaZkN7+7BzvJAEAAACAD00SAAAAAPjQJAEAAACAD00SAAAAAPjQJAEAAACAD00SAAAAAPjQJAEAAACAD00SAAAAAPjQJAEAAACAD00SAAAAAPjQJAEAAACAD00SAAAAAPjQJAEAAACAD00SAAAAAPjQJAEAAACAj+ecc/kuYldqaGhQeXm5Bs36mUIFBYGeE2vwTF8jXWbbhZlCW774Y1svG6+zjZ+NB9/ehiE509i58owp74VttbvWsClftCJiysc32+op3GDbP+li29w2DrCtzeSQpClfWJow5Z2z1ZPNBt/e3IfFprGxY7mIbS3nSrO2L5C1rQVFbMdKfFXUlI/VBa8nXWoaWgVf3mTKF8dTpnwkZNs3H63qYcq7tPH7o9a5tV5VRG3b6yVs5/34OlveMy79THHwDc4aX/9zZbbX0FDMVrz1HF7wbrDrqC2K1hgXg3GppcpsT8jZLgHU0te2P+MbbGstF7Xtn9IPg2frh5uGVrTRti+tx0nEdnmhxkG280LPYRsDZ7MtSb3yzV+qvr5eZWVl283xThIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIBPXpuk2bNn64ADDlBpaal69+6tKVOmaMmSJe0yhx9+uDzPa/dx9tln56liAAAAAN1dXpukRYsWacaMGXr22We1cOFCpdNpTZw4Uc3Nze1yZ5xxhlavXt32cd111+WpYgAAAADdnfFmiDvXggUL2v1//vz56t27t1588UUdeuihbY8XFRWppqams8sDAAAAsAfqUr+TVF9fL0mqqqpq9/if/vQn9ezZU/vuu69mzpyplpaW7Y6RTCbV0NDQ7gMAAAAAgsrrO0l+uVxOF154oQ4++GDtu+++bY+fcsopGjhwoGpra/Xaa6/pRz/6kZYsWaK//e1v2xxn9uzZmjVrVmeVDQAAAKCb6TJN0owZM/TGG2/oqaeeavf4mWee2fbvkSNHqk+fPjrqqKO0dOlSDR06dKtxZs6cqYsvvrjt/w0NDerfv/+uKxwAAABAt9IlmqTzzjtPDz30kBYvXqx+/frtMDt27FhJ0vvvv7/NJikejysej++SOgEAAAB0f3ltkpxzOv/883XvvffqySef1ODBgz/zOa+88ookqU+fPru4OgAAAAB7orw2STNmzNAdd9yh+++/X6WlpVqzZo0kqby8XIWFhVq6dKnuuOMOffWrX1WPHj302muv6aKLLtKhhx6qUaNG5bN0AAAAAN1UXpukuXPnSvrkD8b6zZs3T9OnT1csFtOjjz6q66+/Xs3Nzerfv79OOOEE/fjHP85DtQAAAAD2BHn/cbsd6d+/vxYtWtRJ1QAAAABAF/s7SQAAAACQb13i7nadIVvg5Ap3/M7VFinPNnak1faEXNg2fvO+CVO+JRxsO7fwQsHzfapsf5x3n8q1pvzGZJEpv3RTT1O+pb7ClI802+a2fohtcpsHZ0z5voM2mPKH17xnyheE0qb8O001pnzOBd+fz384wjQ2dmzAyNWmfGHEthY2tdqO3eZkzJQv6Zs05Tc3Bq8nl7Md56mM7ThPpm37Jp2yvTS7jK1+L2X7/qgzvqbImA812LbX+pprZvz2cdGa4PV4xrWWrLQdJ9m4bd+nK3KmfGIv2/VIaz/b3Hpp2/5xJbbX0F1uk+3ckCm1zdfGsdnA2XCx7RyeXlFoyodSprhaa2zbWtiv0ZRvTgQ/VrKJYLXwThIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+ETyXUBnidc2KVyUCZRtbYybxg4VpU35vXpvMOWTWds0lcdaTflXP+wXOLtyZZVp7NVv9zblI02eKV+8yhRX3w+DrYEtUmVhUz5daKu/dIXt+xT1S2tM+T8N7mnKf/ELK0z5TM5W//sv9w+cte1JfJblS6ttT3C2uFdsO7Zci+281pwpNeVVErye6uo609AbX7Wd10aMW2bKL9tkO8+qMGWKJ5eU24ZfYzsaczFTXFlj3sva8uly22KOb7Rtb+OgXOBspMU2dqbYVnvlW7bxG0K2c3g6Yrxs9Gz1u5gtX/K27XqtpU/wuZIkF7HVk+htOw96adv+L3k3GryWnra5itXZ1k620Di3hnOyJLWuKTHlC1carteSiUAx3kkCAAAAAB+aJAAAAADwoUkCAAAAAB+aJAAAAADwoUkCAAAAAB+aJAAAAADwoUkCAAAAAB+aJAAAAADwoUkCAAAAAB+aJAAAAADwoUkCAAAAAB+aJAAAAADwoUkCAAAAAB+aJAAAAADwoUkCAAAAAB+aJAAAAADwieS7gM6SaIorlI0HCzvPNHYmHTbl33mjvylfutQ2/vJBOVM+til4r5yqso0dTtr2ZbrCNn7riBZb3pSWYtGMKd+wqdiUj6yPmvIlH5niGvSAM+U3PjXQlC9ZkTTlB0bTgbMfHR0zjY0dCzfaziPRRtuxWzC6wZRvXF9hyhcNrTflh1VtCJx9f1NP09jpHrbzwjv/GGzKF6437vuNtuM8XGEbv6XWNn66Z/DjXJJkK0dK276/Gyqx1RMakTDl9y4Pvja/W/uMaezrlx5lyq/3bGtZtqlVqMX4vfVetteI0MoCUz5rfJkIJ2yLLVZn296CDbYdWr4sZco39Q2+wTHbKVmNg2y1e7bToEresU1Wpsg2fsvw4Gst1xosyztJAAAAAOBDkwQAAAAAPjRJAAAAAOBDkwQAAAAAPjRJAAAAAOBDkwQAAAAAPjRJAAAAAOBDkwQAAAAAPjRJAAAAAOBDkwQAAAAAPjRJAAAAAOBDkwQAAAAAPjRJAAAAAOBDkwQAAAAAPjRJAAAAAOATyXcBnaXs1bjC8XigbKrcNnayKmfKxzfbetNIqzPlK97yTPl0SfBsYlDGNHa4Om3K9yhtNeU3big15SNxW/2Nq8tM+dIPw6Z89XO27bV+W6OlOtia3yIXsa4d2ykkU8z3ZfKl56u280jO+OrQkqoy5b1KWz1NH9uOxdeWBj+RZ0uzprFDLbbjPFNme41oidiOk5YaU1wlH9ryAx+ynaeiG5tN+XSPYlM+W2Db/40DCkx559nyH1ZXBs5eUTjQNHaqr+01VCW21zjra4rXatv3Lm3M1yZN+cIy29psXW+44JGU6WM7dhPrY6b8xgONE5ANfq4KV6RsYxvFYra1lnjfdr0Wa7BdjxS9G/x6J5sM9vrDFQsAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIBPJN8FdJamL7cqVOQCZXMZW+9YWtliynuDTXH1Kmsw5RtTcVO+2Au2XyRJrQWmsffttcaU71tYZ8pH+2ZN+eUtPUz5f2YHmfKNcdsh1bhP2JRX1jPFvZRhbiWFW2zju5Bte7OlwecrYlv2+Az1Q43fE7MtBeUitrVWtMb2BXIR27HiDJubyNlqyVamTXmlbfs+U2I7r3nOVn/DXrZ8/d62836srsiUd8a1k6q27f+SHvWmfDJpO6+l6w2vuVHbtoZitrVQVJw05ZtXlpryrsC4NsM52/gp27HS0mK73olsNr5mFdnqj2+yHVspZ6vHM5QTXmM7Di1jS1Kyh+0JoQGtpnxLc9SUt1wf5VozgXK8kwQAAAAAPjRJAAAAAOBDkwQAAAAAPjRJAAAAAOCT1yZp9uzZOuCAA1RaWqrevXtrypQpWrJkSbtMIpHQjBkz1KNHD5WUlOiEE07Q2rVr81QxAAAAgO4ur03SokWLNGPGDD377LNauHCh0um0Jk6cqObm5rbMRRddpAcffFB/+ctftGjRIq1atUrHH398HqsGAAAA0J3l9RbgCxYsaPf/+fPnq3fv3nrxxRd16KGHqr6+Xr///e91xx136Mgjj5QkzZs3T/vss4+effZZ/du//dtWYyaTSSWT/7oFZkMD9xEGAAAAEFyX+p2k+vpP/pZBVVWVJOnFF19UOp3WhAkT2jIjRozQgAED9Mwzz2xzjNmzZ6u8vLzto3///ru+cAAAAADdRpdpknK5nC688EIdfPDB2nfffSVJa9asUSwWU0VFRbtsdXW11qzZ9h8pnTlzpurr69s+VqxYsatLBwAAANCN5PXH7fxmzJihN954Q0899dTnGicejyset/0FZgAAAADYoku8k3TeeefpoYce0hNPPKF+/fq1PV5TU6NUKqW6urp2+bVr16qmpqaTqwQAAACwJ8hrk+Sc03nnnad7771Xjz/+uAYPHtzu86NHj1Y0GtVjjz3W9tiSJUv00Ucfady4cZ1dLgAAAIA9QF5/3G7GjBm64447dP/996u0tLTt94zKy8tVWFio8vJy/fu//7suvvhiVVVVqaysTOeff77GjRu3zTvbAQAAAMDnldcmae7cuZKkww8/vN3j8+bN0/Tp0yVJv/rVrxQKhXTCCScomUxq0qRJuummmzq5UgAAAAB7irw2Sc65z8wUFBRozpw5mjNnTidUBAAAAGBP12XubrerfWnQx4oWxwJlBxZtMo3dM9pkyjdmC0z5howtHw9lTPnVifLA2aHlWdPY9Slb7Y+vHG7Kb1pZYcqXfGBb8lXrP7uR94s35Ex5Z/ytwFzUM+Wbam1fINHbtr3ZqrQpH4oZ1k9D2DQ2dmzgYR+a8iHPthbqk7ZjfdXynqa8l7Kt/UhL8LUfTpmGVq7Zdh4pH1Rn+wJGdavKTPmSFbv22ErZylGkyTa3bojtvLNPr7Wm/GFV75ry/7s5+OtWz1izaezWbNSUX58sMeXfbLbdDdg12eqJbLDl45tsayEXDnZdt4UzXvVmSmznwda+tmskF7GNb5GL2l7/i4bWm/J9S23XvhN6v2PKP7puhCm/fG2PwNlcJNhJv0vc3Q4AAAAAugqaJAAAAADwoUkCAAAAAB+aJAAAAADwoUkCAAAAAB+aJAAAAADwoUkCAAAAAB+aJAAAAADwoUkCAAAAAB+aJAAAAADwoUkCAAAAAB+aJAAAAADwoUkCAAAAAB+aJAAAAADwoUkCAAAAAB+aJAAAAADwieS7gM7y3r3DFY4XBMq+HbONnTPuxVzUlncRZ8on+6VNeS+cC15Lq21jS9+15YvXBK9FkoauTpnyUsaUdiHPlM8W2L7vkC6y5ZNltnqs3wbxbLtHXnPY9oT6PeaU0+V88M8Bpny6t+080qum3pSPlNmO3Uyd7cScKQx+3nRx23lHtlOy6laWmfLRettxVbLRdl4o2GDbgJLVtrUQabTlM6W2F8Xcq7a1sKl1oCn/32t7mPLJmtLA2bf2j5vGbvlyqylfVdFkyscLbHPV2mSbq1idbW2GbZurXIkt39rX9iK33z4fmvL1qWDXmVuksrZjff3m4GstU2QbO5Ox5dPG2l9v7GvKF0dsrxEhw7WsAmZ5JwkAAAAAfGiSAAAAAMCHJgkAAAAAfGiSAAAAAMCHJgkAAAAAfGiSAAAAAMCHJgkAAAAAfGiSAAAAAMCHJgkAAAAAfGiSAAAAAMCHJgkAAAAAfGiSAAAAAMCHJgkAAAAAfGiSAAAAAMCHJgkAAAAAfCL5LqCz/Nu3XlGsJBYoG5IzjZ3M2XZjazZqyj/zwWBTPhKy1Z/LeoGzzrONnY2b4srEg9ciSXXDbF8gVWYbP2ebKuXCtnyqKmfKZ8sypny4yJb3jPOrjO37LLlU8Hxok3HnY4cytUlTvqQsYcpvqisx5XMZ27EYbrWtNc9wXss54zqO2Y5ba+1WqXLbcZuqtI1ft4/tWMyFjfnetrXpjOcdhWxrzWuosg2fCj5+0WrT0PJWFpjy6xtt+76iT4Mpn2yx7ftED9va9Gy7XpmqtCkfK02Z8psSRaZ8RUGrKb+irsKUTzcEu46VpEi97do012pbayvKbOf8dZtqTflETdaUDxnWZi4R7GKNd5IAAAAAwIcmCQAAAAB8aJIAAAAAwIcmCQAAAAB8aJIAAAAAwIcmCQAAAAB8aJIAAAAAwIcmCQAAAAB8aJIAAAAAwIcmCQAAAAB8aJIAAAAAwIcmCQAAAAB8aJIAAAAAwIcmCQAAAAB8aJIAAAAAwIcmCQAAAAB8IvkuoLOsOKVSkVA8UNYlk7bBq3ua4pmyAlO+/Eu2fKrcM+WbB2QCZ8Otu7avjrY4U75gc/DaJSkbs9WfqAyb8q29bfveNe7a/ZlL7trxwy228cOtwfdPttC2FrBj4bXBzn9btGyK2b6AbekrnLI9Idpoy4dbg2cjCdvYrb1s6z4bt63ldEXWlI+vs72U5yK2erIFtnzxoHpTPpOxnWczGdv+T2+2vYbKuH9CzYbzmrEUz7YUFDK+RtetKTXlo8bjNhc1nsczxhNJ2ra9qWbbeW1zrNCUX1dfYsqXFtmuN2sHbgycNe5Jrfy4ypT3mm3nnUyRbS1EqhKmvKsMvsVeS7CxeScJAAAAAHxokgAAAADAhyYJAAAAAHxokgAAAADAhyYJAAAAAHxokgAAAADAhyYJAAAAAHxokgAAAADAhyYJAAAAAHxokgAAAADAhyYJAAAAAHxokgAAAADAhyYJAAAAAHxokgAAAADAhyYJAAAAAHwi+S6gs5TdllC0OBco+1FjpWns+tawKV9S0GTKF3mNpnyyudCUDy8vDZwt/sjWVxduCLbPt4g22/LKOVM8Vp825Z3xCMlFbWshF/WMX8C2/52tHIUTtnzEmM8Z9mfWtozxGSJNtrVm3f+ZItuxmCnNmvLZAlv94dbgx0qk2Ti2cd2HE7bxo022E4/1OHfGb4/GN9ieEH7X9hpaut62FuKbM6Z8OJEy5V3Utr1NtbHA2YzxuIpvsuVDWdtaS5VFTfnW3rbj3HLOlyTFbNcAsY22xe/lbPnWjeWmfMi2NLWxqNiUd0XBj5WiqhbT2CU9bPkmr8iUz4Rs+97Vx015RYOvnVwi2MLknSQAAAAA8KFJAgAAAAAfmiQAAAAA8KFJAgAAAACfvDZJixcv1rHHHqva2lp5nqf77ruv3eenT58uz/PafRx99NH5KRYAAADAHiGvTVJzc7P2228/zZkzZ7uZo48+WqtXr277uPPOOzuxQgAAAAB7mp12C/C6ujpVVFSYnjN58mRNnjx5h5l4PK6amprPURkAAAAABNehd5KuvfZa/fnPf277/9SpU9WjRw/17dtXr7766k4rTpKefPJJ9e7dW3vvvbfOOeccbdy4cYf5ZDKphoaGdh8AAAAAEFSHmqSbb75Z/fv3lyQtXLhQCxcu1MMPP6zJkyfrkksu2WnFHX300br99tv12GOP6dprr9WiRYs0efJkZbPb/2Nas2fPVnl5edvHljoBAAAAIIgO/bjdmjVr2pqPhx56SFOnTtXEiRM1aNAgjR07dqcVd9JJJ7X9e+TIkRo1apSGDh2qJ598UkcdddQ2nzNz5kxdfPHFbf9vaGigUQIAAAAQWIfeSaqsrNSKFSskSQsWLNCECRMkSc65Hb7L83kNGTJEPXv21Pvvv7/dTDweV1lZWbsPAAAAAAiqQ+8kHX/88TrllFM0fPhwbdy4se3mCy+//LKGDRu2Uwv0+/jjj7Vx40b16dNnl30NAAAAAHu2DjVJv/rVrzRo0CCtWLFC1113nUpKSiRJq1ev1rnnnht4nKampnbvCi1btkyvvPKKqqqqVFVVpVmzZumEE05QTU2Nli5dqh/+8IcaNmyYJk2a1JGyAQAAAOAzdahJikaj+sEPfrDV4xdddJFpnBdeeEFHHHFE2/+3/C7RtGnTNHfuXL322mu67bbbVFdXp9raWk2cOFE/+clPFI/HO1I2AAAAAHymDv+dpD/+8Y+65ZZb9MEHH+iZZ57RwIEDdf3112vw4ME67rjjAo1x+OGHyzm33c8/8sgjHS0PAAAAADqkQzdumDt3ri6++GJNnjxZdXV1bTdrqKio0PXXX78z6wMAAACATtWhd5J+85vf6Le//a2mTJmia665pu3xMWPGbPPH8LqCkkhCsWguWDaWNI3dnIyZ8iFv+++ebUttSb0pXxZPmPLvJoLX35osMI3t5Wx9eLrEM+WjzbbxwylTXOkiWz3JSls+XWJbC854xHoZWz4XN9Yfto3vOvRtGewMqcpg578tXNS4NkO2vGLGekK2tZk1rLVczDZ2OGnLe7vupq+S7OcRhW35UMZ24Fr3Z7rYNn6k1XbiycV27Yknazlv2naNsgW2J7RU2sZPD2k15ffpv8aU39haZMq3GK+nGgqLTXkv4HXgFs649rNp41qzlaNQYfAX9cpi29z2LGw25dcX2q41NzXa5ioSsZ04C2PpwNlsS1IfB8h16MyxbNky7b///ls9Ho/H1dxs28kAAAAA0JV0qEkaPHiwXnnlla0eX7BggfbZZ5/PWxMAAAAA5E2Hftzu4osv1owZM5RIJOSc03PPPac777xTs2fP1u9+97udXSMAAAAAdJoONUmnn366CgsL9eMf/1gtLS065ZRTVFtbqxtuuEEnnXTSzq4RAAAAADqNuUnKZDK64447NGnSJJ166qlqaWlRU1OTevfuvSvqAwAAAIBOZf6dpEgkorPPPluJxCd3tSgqKqJBAgAAANBtdOjGDQceeKBefvnlnV0LAAAAAORdh34n6dxzz9V//Md/6OOPP9bo0aNVXNz+3uejRo3aKcUBAAAAQGfrUJO05eYM3//+99se8zxPzjl5nqdsdhf/5TwAAAAA2EU61CQtW7ZsZ9cBAAAAAF1Ch5qkgQMH7uw6AAAAAKBL6FCTdPvtt+/w89/97nc7VAwAAAAA5FuHmqQLLrig3f/T6bRaWloUi8VUVFREkwQAAABgt9WhW4Bv3ry53UdTU5OWLFmi8ePH684779zZNQIAAABAp+lQk7Qtw4cP1zXXXLPVu0wAAAAAsDvZaU2SJEUiEa1atWpnDgkAAAAAnapDv5P0wAMPtPu/c06rV6/WjTfeqIMPPninFLazvd/QS5FsPFC2KRkst0Vza8yUb01GTfnSWNKUtyooTAXONvewLZlWZ8u7sCmuaIOxz3e2uFW63PYFMoXGgjxbPtxi2z+5yC7eQd6uHR7blyvL2J5gXQpp47GYMy4GY9xFc8GzBbaNdRHjicq4rZ7xTw26YuPcZmxzZT1PpUts25stsOUzhcb9b+QFXzqSpFS5oX7j2JliWz7ZJ23Kjxyw2pT/Ss+3Tfln6oaY8mtbS035hPF6qrjQdj3V0FRoyucitmPLNRuvkQznkta0bexMga328njClE9lbPV4xuud4ljwa9lMOli2Q03SlClT2v3f8zz16tVLRx55pP7zP/+zI0MCAAAAQJfQoSYplzN+KwQAAAAAdhMd+p2kq666Si0tLVs93traqquuuupzFwUAAAAA+dKhJmnWrFlqamra6vGWlhbNmjXrcxcFAAAAAPnSoSbJOSfP2/qXx1599VVVVVV97qIAAAAAIF9Mv5NUWVkpz/PkeZ722muvdo1SNptVU1OTzj777J1eJAAAAAB0FlOTdP3118s5p+9973uaNWuWysvL2z4Xi8U0aNAgjRs3bqcXCQAAAACdxdQkTZs2TZI0ePBgHXTQQYpGbfenBwAAAICurkO3AD/ssMPa/p1IJJRKtf+jTGVlZZ+vKgAAAADIkw7duKGlpUXnnXeeevfureLiYlVWVrb7AAAAAIDdVYeapEsuuUSPP/645s6dq3g8rt/97neaNWuWamtrdfvtt+/sGgEAAACg03Tox+0efPBB3X777Tr88MN12mmn6ZBDDtGwYcM0cOBA/elPf9Kpp566s+sEAAAAgE7RoXeSNm3apCFDhkj65PePNm3aJEkaP368Fi9evPOqAwAAAIBO1qEmaciQIVq2bJkkacSIEbr77rslffIOU0VFxU4rDgAAAAA6W4eapNNOO02vvvqqJOnSSy/VnDlzVFBQoIsuukiXXHLJTi0QAAAAADqT55xzn3eQDz/8UC+++KKGDRumUaNG7Yy6dpqGhgaVl5dr5Pd+pnCsINBzwgnb14g35kz5SIst74ytbDZue0JrVfB8yaqMaWzPuLqiDanPDvlkC22/VhdK2fZ9tjBsyxv3fS7imfKJCtv4iR7G8XvaJixbalzL4eDjRxps+x47losZD0bb1CpXYHuCl7GtTS9nzGeDZ7PFxo2NGwaXpJTtuPWM+XBLh77fGZhlX0pSpsz4mthoq9/ZloLCSdsTMiW2+nOGlyFrLVnjcSXjvon2bjXlB/faaMoXRWyv6QVh2zXG6pZd+ydn1jWUmPLJpO3vh2YTxmuYaPCDMZfZtecF63nN/LZMxLb2wxtigbO5REIf/vj/qb6+fod/tqhDN27wSyQSGjhwoAYOHPh5hwIAAACAvOtQm5nNZvWTn/xEffv2VUlJiT744ANJ0mWXXabf//73O7VAAAAAAOhMHWqSfvazn2n+/Pm67rrrFIv96+2tfffdV7/73e92WnEAAAAA0Nk61CTdfvvtuvXWW3XqqacqHP7X7w3st99+euedd3ZacQAAAADQ2TrUJK1cuVLDhg3b6vFcLqd0Ov25iwIAAACAfOlQk/SFL3xB//u//7vV4/fcc4/233//z10UAAAAAORLh+5ud/nll2vatGlauXKlcrmc/va3v2nJkiW6/fbb9dBDD+3sGgEAAACg05jeSfrggw/knNNxxx2nBx98UI8++qiKi4t1+eWX6+2339aDDz6or3zlK7uqVgAAAADY5UzvJA0fPlyrV69W7969dcghh6iqqkqvv/66qqurd1V9AAAAANCpTO8kOdf+L7Y//PDDam5u3qkFAQAAAEA+dejGDVt8umkCAAAAgN2dqUnyPE+e5231GAAAAAB0F6bfSXLOafr06YrH45KkRCKhs88+W8XFxe1yf/vb33ZehQAAAADQiUxN0rRp09r9/9vf/vZOLQYAAAAA8s3UJM2bN29X1QEAAAAAXcLnunEDAAAAAHQ3pneSdmcNB7cqVBTsbny1PepNY1cUtJryRZGUKf/6mj6mfCoZNeWzm+OBs5v2t/XVsY1hU95zttojLaa4ssE3VZLkZWz5TMA1tkW6KmfKu3DWlFfENr5ythuxxMqTpnw8ng6cbW2oMI2NHQvX2A6WaNS21mIRWz4Usq3N5lbbwRsKBT8Wy4tt5/DqoiZTflVTmSm/Yb0t71LGc36pca6KbCfCkPF+TulC2+uEVdZ4HowVBj9PSVL/HnWBsyHZXiM2tBSZ8q3JmClfVpQw5a3eWWf7O5qplO2yNFtv214vY1uc0QbbNY91JYeMb1WEDfMbs13KyjNeXuSMHYT1+itnO60pYljK2WSwdcA7SQAAAADgQ5MEAAAAAD40SQAAAADgQ5MEAAAAAD40SQAAAADgQ5MEAAAAAD40SQAAAADgQ5MEAAAAAD40SQAAAADgQ5MEAAAAAD40SQAAAADgQ5MEAAAAAD40SQAAAADgQ5MEAAAAAD40SQAAAADgE8l3AZ1mbYFUUBAo+nHa1juujpab8l4oZ8pHIra8c54tH3aBs4UrbUumcF3wsSUplDbFlTOu4FzEtm+yhbbxXcg2vrdx136fIhsPm/KZ8qwtn7aNH48bJxg7zeBem0z5eCRjyveKN5nyIc92btiYLDLlG9PBzveSFJKtlg2txab8xs0lprwabSe2XNxWv6K215RcU9SUj9TbzgtR22lHhets59l4nW3/FK211d9aVBs4m6iynfMTPW3b2jrQdtymSm1zmy6z1V9SmDTle/awnacqBraa8qmcbW6XbuppytfX285TobVxUz5dGXx+E/1tx3m0OGWrpTlmynvNtn3vIrbj1isJvm9yrYlAOd5JAgAAAAAfmiQAAAAA8KFJAgAAAAAfmiQAAAAA8Mlrk7R48WIde+yxqq2tled5uu+++9p93jmnyy+/XH369FFhYaEmTJig9957Lz/FAgAAANgj5LVJam5u1n777ac5c+Zs8/PXXXedfv3rX+vmm2/WP//5TxUXF2vSpElKJILdlQIAAAAArPJ6C/DJkydr8uTJ2/ycc07XX3+9fvzjH+u4446TJN1+++2qrq7Wfffdp5NOOmmbz0smk0om/3XLyYaGhp1fOAAAAIBuq8v+TtKyZcu0Zs0aTZgwoe2x8vJyjR07Vs8888x2nzd79myVl5e3ffTv378zygUAAADQTXTZJmnNmjWSpOrq6naPV1dXt31uW2bOnKn6+vq2jxUrVuzSOgEAAAB0L3n9cbtdIR6PKx63/QVjAAAAANiiy76TVFNTI0lau3Ztu8fXrl3b9jkAAAAA2Nm6bJM0ePBg1dTU6LHHHmt7rKGhQf/85z81bty4PFYGAAAAoDvL64/bNTU16f3332/7/7Jly/TKK6+oqqpKAwYM0IUXXqif/vSnGj58uAYPHqzLLrtMtbW1mjJlSv6KBgAAANCt5bVJeuGFF3TEEUe0/f/iiy+WJE2bNk3z58/XD3/4QzU3N+vMM89UXV2dxo8frwULFqigoCBfJQMAAADo5vLaJB1++OFyzm33857n6aqrrtJVV13ViVUBAAAA2JN12d9JAgAAAIB86Ha3AN+eaH1I4UTAnnCT8cf5PFs8nLLltf0327apYrPtCUXrs4GzJW+v/eyQj9fcasq7ZNI2vvFHL115iSmfLbXdTj5dFjPlc2Hb4mmpth2yrb1s4ydzYVM+12jLt2YN88W3cHaqjx8bYMrnjK8O79niykVt56lw0raWw4ZTSdb6VyOs5/wi27Y649rPlAY/h0uS12ib3HCLrSDj7jHPrXX/xBpzpnymyPYFsrHg9RtPsQqlbfmiD21z6zlbvrHM9pqb7m3bgPrSQlM+m7XNVW6D7WCPNNvGL2ywreV0qfHcEPQ6VpKKbfs+3WS7fvEStsUcrbfty5hxX2YKg6/lbMDXBy5DAAAAAMCHJgkAAAAAfGiSAAAAAMCHJgkAAAAAfGiSAAAAAMCHJgkAAAAAfGiSAAAAAMCHJgkAAAAAfGiSAAAAAMCHJgkAAAAAfGiSAAAAAMCHJgkAAAAAfGiSAAAAAMCHJgkAAAAAfGiSAAAAAMAnku8COks4KYUDZp2xdUxVOFs+Ysu7mqQp3xqyjb+x1bAMJvcwje2lPVM+3MO2rdFYxpQvL2415SXb+GGvyZQviydM+b2LN5vyFVHb9oaVM+V3pb8sPDjfJXQrtUetMOWjoawpH/Js553WTNSUX7mp3JTPZIOfyJ2znaeseatsi/GlOWusxxjPlNnWgoyvccaXCSUytie09An66v+JaOOum99Ere01pahXsyk/sma1Kb+2pdSUT2Vt+3LV8p6mfGZlkSkfa7BdsGVjtrWZixvXsu00JdvoUnxj8O1NJwtMY+fKbcd5QY1tbWZ62uaqqTlmynuJ4Gsz1xpsW3knCQAAAAB8aJIAAAAAwIcmCQAAAAB8aJIAAAAAwIcmCQAAAAB8aJIAAAAAwIcmCQAAAAB8aJIAAAAAwIcmCQAAAAB8aJIAAAAAwIcmCQAAAAB8aJIAAAAAwIcmCQAAAAB8aJIAAAAAwIcmCQAAAAB8aJIAAAAAwCeS7wI6y89P+72KS8OBsr3CzaaxEy7YuFukjXmrjbliU/6+jaMDZ19Z39c09qY6Wy3OlJaSrVFTfm1LzJT3PFtFobAtvylWZMov31hlyjvnmfKhUM6UtyqIpXfp+Ni+jc22tbar1040bMxHs6Z8SzL4y1uu0XYe8bK2feOlbflYqy1fsN6Wj7TazlPZuG1860tc2rY0JVs5SvSyrbVErW2tKRp8/FDMNnY6ZbtMe2t9tSnfuL7ElI9sstVTunbXrk3rVUO62FZPutS69m31hI3nBi8TPBurM5536m1zm9lYaspb35aJGbZVknLx4FkvEawY3kkCAAAAAB+aJAAAAADwoUkCAAAAAB+aJAAAAADwoUkCAAAAAB+aJAAAAADwoUkCAAAAAB+aJAAAAADwoUkCAAAAAB+aJAAAAADwoUkCAAAAAB+aJAAAAADwoUkCAAAAAB+aJAAAAADwoUkCAAAAAJ9IvgvoLJfe9O8KxwsCZXNR29hezpa3jp8zzlKmxJny8Y1e4GyPt9KmsXu/scqUz65db8p7YVufn0skTHmrcFmZKe8VF5nyrtI2fq4oZspLwddCR+RihYGzDVN2XR17pL9XmeLWlZAL2Z6RcrbzVMx4ni1qCZ4Np2y1JKps25qstOZt9TQOtu2ccMI6u7Z6og279jySKbLV4+K2/RMpS5nymYbg51lvk+0CwLOVopZ48HOsJEWytrkK2S4BlOxhm6t02lZPtsA2vnLGevrYJiAUta21eIFthyZTwS8I43HjZBlVFdmup7I52/VaNmc8b6aDH1vZlmSgHO8kAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+ETyXUBnKZ+8WpHieKDsxuYi09gtLcHGbeM5UzxkzGeStml14Vjg7CYXNY2drOhvynvZfqa8lQt7pryXs43v5WxzZR4/a1w7KVt+VwsnjRuMnab38w2mvJfO2vLJjCnvCoOfdyQpW2g792SLgp8Hne20oGhzeJfmQ2lbQYmeprj1JUjRBls9kWbb+DLuf884YV7Otv/dpkJTPm5Y+uFW48YahVusO9MWz8ZtiydnO8yVKbK9RriwrR5XaBs/HLedB0Mh2/jJhO28VliUCpwtigfPSlIsbNvW+tYCUz6TsR2HkYitnl2Bd5IAAAAAwIcmCQAAAAB8aJIAAAAAwKdLN0lXXnmlPM9r9zFixIh8lwUAAACgG+vyN2744he/qEcffbTt/5FIly8ZAAAAwG6sy3cckUhENTU1+S4DAAAAwB6iS/+4nSS99957qq2t1ZAhQ3Tqqafqo48+2mE+mUyqoaGh3QcAAAAABNWlm6SxY8dq/vz5WrBggebOnatly5bpkEMOUWNj43afM3v2bJWXl7d99O9v+zs9AAAAAPZsXbpJmjx5sk488USNGjVKkyZN0t///nfV1dXp7rvv3u5zZs6cqfr6+raPFStWdGLFAAAAAHZ3Xf53kvwqKiq011576f33399uJh6PKx6Pd2JVAAAAALqTLv1O0qc1NTVp6dKl6tOnT75LAQAAANBNdekm6Qc/+IEWLVqk5cuX6+mnn9Y3vvENhcNhnXzyyfkuDQAAAEA31aV/3O7jjz/WySefrI0bN6pXr14aP368nn32WfXq1SvfpQEAAADoprp0k3TXXXfluwQAAAAAe5gu/eN2AAAAANDZuvQ7STvT2voShdMFgbKhkDONXVnebMpXl2z/7zxtS59C2x/EDXk5U35JbXXg7KqN5aaxGzcG2+dbeCnPlHcR21y54owpHy9NmvKxmG38gqgt75xt/6QyYVs+bTslZDO277Nks8HzoVWmofEZlpxZZHuCca3JdijKyxrHN+Ytp8FQ2jh21hRXLmrbObkC2zk83GQ7DmObbdtbtNZWf9Z4g9lkpa2e1hrbBHhVKVO+pCRhyltkc7a5yuVs+8Z6/eJ5xrlNGS8bDed8SYqEjWvfmI8bX3NzxvOgdX5d1HgyMQiHbPvGynr9kjSutYzx+sVyrGQzwbK8kwQAAAAAPjRJAAAAAOBDkwQAAAAAPjRJAAAAAOBDkwQAAAAAPjRJAAAAAOBDkwQAAAAAPjRJAAAAAOBDkwQAAAAAPjRJAAAAAOBDkwQAAAAAPjRJAAAAAOBDkwQAAAAAPjRJAAAAAOBDkwQAAAAAPjRJAAAAAOATyXcBnSXVGFcoEw+U9WI509itdQWm/IZwqSn/ZrK/KS9ni3tZL3A20mjrq4vqg48tSdEGU1zhtG1jMwVhUz5ZGTPlmytsa6ex0JaXbXdKUdv4njEfitjyhUWpwNmkbMcVdmzCl94y5Vuz0V1UySdaMrbxM8527KaywfMhz3YeyTnbgZjO2WpvSNjWfiZrOy8n07aX/paQcf/kbPunIJY25auLWk354mjw844khawvogb1KdvcJjK2udrcWGTKW+cqm7atZatsynZesK0cKeHZXtMjsaztCxjPJVbZTPD939ps29acdW4N146SJOP1gme93jHs+1wi2LzyThIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+NAkAQAAAIAPTRIAAAAA+ETyXUBnia6PKlQQDRY2to5e1l6PRSjt2Z6QM46fCZ4tXOdMYxdtMAwuKVZvyzvPuG+MPGfbXhey1ePCxrxxc631SGHb+GHbKcTLBjwGJa08wjQ0PsObvxxpyjvbUlCmwLbWMoW2fC5miitdbKilxHacp8ttJ/1QadqUj8Zs58Fw2HbSt+Y9z7Z/4lHb/snlbC+6m5qLTPl1uRJTPmTc3qyh/kzGtq3JhrgpH2qynZNDCdtxaP3Oei5mfA0tNl5QxYxrP2qrJ5O07U+Xtu0hL2nco5bNtU5W2LZvZIwra7y+MF7LWup3iWC18E4SAAAAAPjQJAEAAACAD00SAAAAAPjQJAEAAACAD00SAAAAAPjQJAEAAACAD00SAAAAAPjQJAEAAACAD00SAAAAAPjQJAEAAACAD00SAAAAAPjQJAEAAACAD00SAAAAAPjQJAEAAACAD00SAAAAAPjQJAEAAACATyTfBXSWIXdsUCQcD5TNFQXLtTG2ml7W2Z7gbHkXs01rS9/CwNlUiW1jm3uHTfnWSls+lDXFFU7Z9qU17zxTXLmo7QnpIls+U2jL54xnBGebLoXSptFtg2OHWqptx27WeBq0rgUvZxzfeGxZxvcyxrHTxuOqxXZgmQ4TSdlVBaZ8KGkbP9Zg297GKuN507h2XNj6GmqLm1kXp0HEuqnGfZMtNM5V3Hjgxm0v0vHilCmfy9rOaznjXLmccW6ta7PEePIx8MLGuTLuS+OlqZSxXiwbx98FeCcJAAAAAHxokgAAAADAhyYJAAAAAHxokgAAAADAhyYJAAAAAHxokgAAAADAhyYJAAAAAHxokgAAAADAhyYJAAAAAHxokgAAAADAhyYJAAAAAHxokgAAAADAhyYJAAAAAHxokgAAAADAhyYJAAAAAHwi+S6gs3z81V4KxwsCZXPWveLZ4i5sy1vryUWdKZ+uyAbOxns2m8buU9lgyhdG0qZ8OmfbmWEvZ8oXhDOmfFEkZcqXRhOmfMa4vY2ZuCnflLblW9IxUz4cCr7/m1/qaxobO9bj6x+b8hHjsRINBz+PdIZ0NvixkjIeV4mM7aScTNvyza224zDbr9WWN6UlV2A7L9vOCpKzvWQplYya8tmM7fvBLml8kc4GvwjwUrYLBs8wtiQZD1t5xsUQajbuyxZbPr3ZNreece2EE7b9GTXPlykuGesPGS5JQrbD1jR2pzDuG2dYatlksIninSQAAAAA8KFJAgAAAAAfmiQAAAAA8KFJAgAAAACf3aJJmjNnjgYNGqSCggKNHTtWzz33XL5LAgAAANBNdfkm6c9//rMuvvhiXXHFFXrppZe03377adKkSVq3bl2+SwMAAADQDXX5JumXv/ylzjjjDJ122mn6whe+oJtvvllFRUX6wx/+sM18MplUQ0NDuw8AAAAACKpLN0mpVEovvviiJkyY0PZYKBTShAkT9Mwzz2zzObNnz1Z5eXnbR//+/TurXAAAAADdQJdukjZs2KBsNqvq6up2j1dXV2vNmjXbfM7MmTNVX1/f9rFixYrOKBUAAABAN2H7M+C7gXg8rnjc9tfKAQAAAGCLLv1OUs+ePRUOh7V27dp2j69du1Y1NTV5qgoAAABAd9alm6RYLKbRo0frsccea3ssl8vpscce07hx4/JYGQAAAIDuqsv/uN3FF1+sadOmacyYMTrwwAN1/fXXq7m5Waeddlq+SwMAAADQDXX5Julb3/qW1q9fr8svv1xr1qzRl770JS1YsGCrmzkAAAAAwM7Q5ZskSTrvvPN03nnn5bsMAAAAAHuA3aJJ+jycc5KkbDIR+Dm5rPGLeLa4C9vyuYxx/Kyzjd8afIOzLcH3oyRlYklbPpK25XO2nem8nCmfDtsWQzqSMuVTxrx1e9NZ2+LM2Ha/MmnbWnOh4Ps/l7CtNexYptl2LMp4rChszO9imWzwX7m1HlfZjO28YM3nWo3ncMO2dkTW+iJk5Gybq1zSuD+N+8cljS/ShvOsl7Kdk72cNW+KyxmvXzzjXFnHt7LW4yVsBTnzfJniknV/Gg5FZ3w9t4zdKaz7xnCYZ1OfXF+4zzj5eO6zEru5jz/+mD8oCwAAAKDNihUr1K9fv+1+vts3SblcTqtWrVJpaak871/fEWhoaFD//v21YsUKlZWV5bFC7GzMbffF3HZvzG/3xdx2X8xt99Vd59Y5p8bGRtXW1ioU2v5bUN3+x+1CodAOu8SysrJuNfH4F+a2+2Juuzfmt/tibrsv5rb76o5zW15e/pmZLv13kgAAAACgs9EkAQAAAIDPHtskxeNxXXHFFYrH4/kuBTsZc9t9MbfdG/PbfTG33Rdz233t6XPb7W/cAAAAAAAWe+w7SQAAAACwLTRJAAAAAOBDkwQAAAAAPjRJAAAAAOCzRzZJc+bM0aBBg1RQUKCxY8fqueeey3dJ6IDFixfr2GOPVW1trTzP03333dfu8845XX755erTp48KCws1YcIEvffee/kpFiazZ8/WAQccoNLSUvXu3VtTpkzRkiVL2mUSiYRmzJihHj16qKSkRCeccILWrl2bp4oR1Ny5czVq1Ki2P044btw4Pfzww22fZ167j2uuuUae5+nCCy9se4z53T1deeWV8jyv3ceIESPaPs+87t5Wrlypb3/72+rRo4cKCws1cuRIvfDCC22f31Ovp/a4JunPf/6zLr74Yl1xxRV66aWXtN9++2nSpElat25dvkuDUXNzs/bbbz/NmTNnm5+/7rrr9Otf/1o333yz/vnPf6q4uFiTJk1SIpHo5EphtWjRIs2YMUPPPvusFi5cqHQ6rYkTJ6q5ubktc9FFF+nBBx/UX/7yFy1atEirVq3S8ccfn8eqEUS/fv10zTXX6MUXX9QLL7ygI488Uscdd5zefPNNScxrd/H888/rlltu0ahRo9o9zvzuvr74xS9q9erVbR9PPfVU2+eY193X5s2bdfDBBysajerhhx/WW2+9pf/8z/9UZWVlW2aPvZ5ye5gDDzzQzZgxo+3/2WzW1dbWutmzZ+exKnxekty9997b9v9cLudqamrcz3/+87bH6urqXDwed3feeWceKsTnsW7dOifJLVq0yDn3yVxGo1H3l7/8pS3z9ttvO0numWeeyVeZ6KDKykr3u9/9jnntJhobG93w4cPdwoUL3WGHHeYuuOAC5xzH7e7siiuucPvtt982P8e87t5+9KMfufHjx2/383vy9dQe9U5SKpXSiy++qAkTJrQ9FgqFNGHCBD3zzDN5rAw727Jly7RmzZp2c11eXq6xY8cy17uh+vp6SVJVVZUk6cUXX1Q6nW43vyNGjNCAAQOY391INpvVXXfdpebmZo0bN4557SZmzJihY445pt08Shy3u7v33ntPtbW1GjJkiE499VR99NFHkpjX3d0DDzygMWPG6MQTT1Tv3r21//7767e//W3b5/fk66k9qknasGGDstmsqqur2z1eXV2tNWvW5Kkq7Apb5pO53v3lcjldeOGFOvjgg7XvvvtK+mR+Y7GYKioq2mWZ393D66+/rpKSEsXjcZ199tm699579YUvfIF57QbuuusuvfTSS5o9e/ZWn2N+d19jx47V/PnztWDBAs2dO1fLli3TIYccosbGRuZ1N/fBBx9o7ty5Gj58uB555BGdc845+v73v6/bbrtN0p59PRXJdwEAsCMzZszQG2+80e7n37F723vvvfXKK6+ovr5e99xzj6ZNm6ZFixbluyx8TitWrNAFF1yghQsXqqCgIN/lYCeaPHly279HjRqlsWPHauDAgbr77rtVWFiYx8rweeVyOY0ZM0ZXX321JGn//ffXG2+8oZtvvlnTpk3Lc3X5tUe9k9SzZ0+Fw+Gt7riydu1a1dTU5Kkq7Apb5pO53r2dd955euihh/TEE0+oX79+bY/X1NQolUqprq6uXZ753T3EYjENGzZMo0eP1uzZs7XffvvphhtuYF53cy+++KLWrVunL3/5y4pEIopEIlq0aJF+/etfKxKJqLq6mvntJioqKrTXXnvp/fff57jdzfXp00df+MIX2j22zz77tP045Z58PbVHNUmxWEyjR4/WY4891vZYLpfTY489pnHjxuWxMuxsgwcPVk1NTbu5bmho0D//+U/mejfgnNN5552ne++9V48//rgGDx7c7vOjR49WNBptN79LlizRRx99xPzuhnK5nJLJJPO6mzvqqKP0+uuv65VXXmn7GDNmjE499dS2fzO/3UNTU5OWLl2qPn36cNzu5g4++OCt/sTGu+++q4EDB0raw6+n8n3niM521113uXg87ubPn+/eeustd+aZZ7qKigq3Zs2afJcGo8bGRvfyyy+7l19+2Ulyv/zlL93LL7/sPvzwQ+ecc9dcc42rqKhw999/v3vttdfccccd5wYPHuxaW1vzXDk+yznnnOPKy8vdk08+6VavXt320dLS0pY5++yz3YABA9zjjz/uXnjhBTdu3Dg3bty4PFaNIC699FK3aNEit2zZMvfaa6+5Sy+91Hme5/7nf/7HOce8djf+u9s5x/zurv7jP/7DPfnkk27ZsmXuH//4h5swYYLr2bOnW7dunXOOed2dPffccy4Sibif/exn7r333nN/+tOfXFFRkfuv//qvtsyeej21xzVJzjn3m9/8xg0YMMDFYjF34IEHumeffTbfJaEDnnjiCSdpq49p06Y55z65beVll13mqqurXTwed0cddZRbsmRJfotGINuaV0lu3rx5bZnW1lZ37rnnusrKSldUVOS+8Y1vuNWrV+evaATyve99zw0cONDFYjHXq1cvd9RRR7U1SM4xr93Np5sk5nf39K1vfcv16dPHxWIx17dvX/etb33Lvf/++22fZ153bw8++KDbd999XTwedyNGjHC33npru8/vqddTnnPO5ec9LAAAAADoevao30kCAAAAgM9CkwQAAAAAPjRJAAAAAOBDkwQAAAAAPjRJAAAAAOBDkwQAAAAAPjRJAAAAAOBDkwQAAAAAPjRJAIBuYfr06ZoyZUq+ywAAdAORfBcAAMBn8Txvh5+/4oordMMNN8g510kVAQC6M5okAECXt3r16rZ///nPf9bll1+uJUuWtD1WUlKikpKSfJQGAOiG+HE7AECXV1NT0/ZRXl4uz/PaPVZSUrLVj9sdfvjhOv/883XhhReqsrJS1dXV+u1vf6vm5maddtppKi0t1bBhw/Twww+3+1pvvPGGJk+erJKSElVXV+s73/mONmzY0MlbDADIJ5okAEC3ddttt6lnz5567rnndP755+ucc87RiSeeqIMOOkgvvfSSJk6cqO985ztqaWmRJNXV1enII4/U/vvvrxdeeEELFizQ2rVrNXXq1DxvCQCgM9EkAQC6rf32208//vGPNXz4cM2cOVMFBQXq2bOnzjjjDA0fPlyXX365Nm7cqNdee02SdOONN2r//ffX1VdfrREjRmj//ffXH/7wBz3xxBN6991387w1AIDOwu8kAQC6rVGjRrX9OxwOq0ePHho5cmTbY9XV1ZKkdevWSZJeffVVPfHEE9v8/aalS5dqr7322sUVAwC6ApokAEC3FY1G2/3f87x2j225a14ul5MkNTU16dhjj9W111671Vh9+vTZhZUCALoSmiQAAP7Pl7/8Zf31r3/VoEGDFInwEgkAeyp+JwkAgP8zY8YMbdq0SSeffLKef/55LV26VI888ohOO+00ZbPZfJcHAOgkNEkAAPyf2tpa/eMf/1A2m9XEiRM1cuRIXXjhhaqoqFAoxEsmAOwpPMefJwcAAACANnxbDAAAAAB8aJIAAAAAwIcmCQAAAAB8aJIAAAAAwIcmCQAAAAB8aJIAAAAAwIcmCQAAAAB8aJIAAAAAwIcmCQAAAAB8aJIAAAAAwIcmCQAAAAB8/j9ojKS8fcGxVgAAAABJRU5ErkJggg==\n"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Testing code to check if your validation data loaders are working\nall = []\nfor i, data in enumerate(val_loader):\n    frames, phoneme = data\n    all.append(phoneme)\n    break","metadata":{"id":"dJTrLe7J5dSc","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:21:18.197187Z","iopub.execute_input":"2026-02-02T07:21:18.197802Z","iopub.status.idle":"2026-02-02T07:21:18.229175Z","shell.execute_reply.started":"2026-02-02T07:21:18.197757Z","shell.execute_reply":"2026-02-02T07:21:18.228468Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Network Architecture\n","metadata":{"id":"Nxjwve20JRJ2"}},{"cell_type":"markdown","source":"This section defines your network architecture for the homework. We have given you a sample architecture that can easily clear the very low cutoff for the early submission deadline.","metadata":{"id":"3NJzT-mRw6iy"}},{"cell_type":"code","source":"# This architecture will make you cross the very low cutoff\n# However, you need to run a lot of experiments to cross the medium or high cutoff\n\nclass Network(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(Network, self).__init__()\n\n        self.model = nn.Sequential(\n            torch.nn.Linear(input_size, 2048),\n            torch.nn.ReLU(),\n\n            torch.nn.Linear(2048, 2048),\n            torch.nn.ReLU(),\n\n            torch.nn.Linear(2048, 2048),\n            torch.nn.ReLU(),\n\n            torch.nn.Linear(2048, 512),\n            torch.nn.ReLU(),\n\n            torch.nn.Linear(512, output_size)\n        )\n\n        if config['weight_initialization'] is not None:\n            self.initialize_weights()\n\n    def initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, torch.nn.Linear):\n                if config[\"weight_initialization\"] == \"xavier_normal\":\n                    torch.nn.init.xavier_normal_(m.weight)\n                elif config[\"weight_initialization\"] == \"xavier_uniform\":\n                    torch.nn.init.xavier_uniform_(m.weight)\n                elif config[\"weight_initialization\"] == \"kaiming_normal\":\n                    torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n                elif config[\"weight_initialization\"] == \"kaiming_uniform\":\n                    torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n                elif config[\"weight_initialization\"] == \"uniform\":\n                    torch.nn.init.uniform_(m.weight)\n                else:\n                    raise ValueError(\"Invalid weight_initialization value\")\n\n                # Initialize bias to 0\n                m.bias.data.fill_(0)\n\n\n    def forward(self, x):\n\n        # Flatten to a 1D vector for each data point\n        x = torch.flatten(x, start_dim=1)  # Keeps batch size, flattens the rest\n\n        return self.model(x)","metadata":{"id":"-YsMpN-Exafq","trusted":true,"execution":{"iopub.status.busy":"2026-02-01T11:32:33.249227Z","iopub.execute_input":"2026-02-01T11:32:33.249474Z","iopub.status.idle":"2026-02-01T11:32:33.258424Z","shell.execute_reply.started":"2026-02-01T11:32:33.249451Z","shell.execute_reply":"2026-02-01T11:32:33.257819Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class ResidualMLPBlock(nn.Module):\n    def __init__(self, dim, bottleneck, dropout):\n        super().__init__()\n\n        self.fc1 = nn.Linear(dim, bottleneck)\n        self.fc2 = nn.Linear(bottleneck, dim)\n\n        self.norm1 = nn.LayerNorm(bottleneck)\n        self.norm2 = nn.LayerNorm(dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        identity = x\n\n        # Project to bottleneck\n        x = self.fc1(x)\n        x = self.norm1(x)\n        x = F.gelu(x)\n\n        # Project back\n        x = self.fc2(x)\n        x = self.norm2(x)\n\n        x = x + identity\n        return self.dropout(F.gelu(x))\n\n\n\nclass Network(nn.Module):\n    def __init__(self, input_size, output_size, dropout_rate=0.1):\n        super().__init__()\n\n        hidden = 1536\n        bottleneck = 640\n        num_blocks = 6  \n\n        # Input projection\n        self.fc_in = nn.Linear(input_size, hidden)\n        self.norm_in = nn.LayerNorm(hidden)\n\n        # Residual trunk\n        self.blocks = nn.Sequential(\n            *[ResidualMLPBlock(hidden, bottleneck, dropout_rate) \n              for _ in range(num_blocks)]\n        )\n\n        # Deep head\n        self.head = nn.Sequential(\n            nn.Linear(hidden, 1024),\n            nn.GELU(),\n            nn.LayerNorm(1024),\n\n            nn.Linear(1024, 512),\n            nn.GELU(),\n            nn.LayerNorm(512),\n\n            nn.Linear(512, output_size)\n        )\n\n        if config.get(\"weight_initialization\", None) is not None:\n            self.initialize_weights()\n\n    def initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                if config[\"weight_initialization\"] == \"xavier_normal\":\n                    nn.init.xavier_normal_(m.weight)\n                elif config[\"weight_initialization\"] == \"xavier_uniform\":\n                    nn.init.xavier_uniform_(m.weight)\n                elif config[\"weight_initialization\"] == \"kaiming_normal\":\n                    nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n                elif config[\"weight_initialization\"] == \"kaiming_uniform\":\n                    nn.init.kaiming_uniform_(m.weight, nonlinearity=\"relu\")\n                elif config[\"weight_initialization\"] == \"uniform\":\n                    nn.init.uniform_(m.weight)\n                else:\n                    raise ValueError(\"Invalid weight_initialization value\")\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = torch.flatten(x, start_dim=1)\n\n        x = self.fc_in(x)\n        x = self.norm_in(x)\n        x = F.gelu(x)\n\n        x = self.blocks(x)\n        return self.head(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:21:22.253852Z","iopub.execute_input":"2026-02-02T07:21:22.254562Z","iopub.status.idle":"2026-02-02T07:21:22.264886Z","shell.execute_reply.started":"2026-02-02T07:21:22.254532Z","shell.execute_reply":"2026-02-02T07:21:22.264177Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Define Model, Loss Function and Optimizer","metadata":{"id":"HejoSXe3vMVU"}},{"cell_type":"markdown","source":"Here we define the model, loss function, optimizer and optionally a learning rate scheduler.","metadata":{"id":"xAhGBH7-xxth"}},{"cell_type":"code","source":"# Define the input size\nINPUT_SIZE  = (2*config['context'] + 1) * 28 # Why is this the case?\n\n# Instantiate model and load to GPU\nmodel       = Network(INPUT_SIZE, len(train_data.phonemes)).to(device).cuda()\n\n# Remember, you are limited to 20 million parameters for HW1 (including ensembles)\n# Check to stay below 20 MIL Parameter limit\nassert sum(p.numel() for p in model.parameters() if p.requires_grad) < 20_000_000, \"Exceeds 20 MIL params. Any submission made to Kaggle with this model will be flagged as an AIV.\"","metadata":{"id":"_qtrEM1ZvLje","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:21:26.339897Z","iopub.execute_input":"2026-02-02T07:21:26.340371Z","iopub.status.idle":"2026-02-02T07:21:26.578347Z","shell.execute_reply.started":"2026-02-02T07:21:26.340341Z","shell.execute_reply":"2026-02-02T07:21:26.577769Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Inspect model architecture and check to verify number of parameters of your network\ntry:\n    # Install and import torchsummaryX\n    !pip install torchsummaryX==1.1.0\n    from torchsummaryX import summary\n\n    summary(model, frames.to(device))\n\nexcept:\n    !pip install torchsummary\n    from torchsummary import summary\n\n    summary(model, frames[0].to(device).shape)","metadata":{"id":"JPWu0H557ev4","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:21:29.057253Z","iopub.execute_input":"2026-02-02T07:21:29.057992Z","iopub.status.idle":"2026-02-02T07:21:33.081405Z","shell.execute_reply.started":"2026-02-02T07:21:29.057960Z","shell.execute_reply":"2026-02-02T07:21:33.080668Z"}},"outputs":[{"name":"stdout","text":"Collecting torchsummaryX==1.1.0\n  Downloading torchsummaryX-1.1.0-py3-none-any.whl.metadata (261 bytes)\nDownloading torchsummaryX-1.1.0-py3-none-any.whl (2.9 kB)\nInstalling collected packages: torchsummaryX\nSuccessfully installed torchsummaryX-1.1.0\n----------------------------------------------------------------------------------------------------\nLayer                   Kernel Shape         Output Shape         # Params (K)      # Mult-Adds (M)\n====================================================================================================\n0_Linear                [1820, 1536]         [2048, 1536]             2,797.06                 2.80\n1_LayerNorm                   [1536]         [2048, 1536]                 3.07                 0.00\n2_Linear                 [1536, 640]          [2048, 640]               983.68                 0.98\n3_LayerNorm                    [640]          [2048, 640]                 1.28                 0.00\n4_Linear                 [640, 1536]         [2048, 1536]               984.58                 0.98\n5_LayerNorm                   [1536]         [2048, 1536]                 3.07                 0.00\n6_Dropout                          -         [2048, 1536]                    -                    -\n7_Linear                 [1536, 640]          [2048, 640]               983.68                 0.98\n8_LayerNorm                    [640]          [2048, 640]                 1.28                 0.00\n9_Linear                 [640, 1536]         [2048, 1536]               984.58                 0.98\n10_LayerNorm                  [1536]         [2048, 1536]                 3.07                 0.00\n11_Dropout                         -         [2048, 1536]                    -                    -\n12_Linear                [1536, 640]          [2048, 640]               983.68                 0.98\n13_LayerNorm                   [640]          [2048, 640]                 1.28                 0.00\n14_Linear                [640, 1536]         [2048, 1536]               984.58                 0.98\n15_LayerNorm                  [1536]         [2048, 1536]                 3.07                 0.00\n16_Dropout                         -         [2048, 1536]                    -                    -\n17_Linear                [1536, 640]          [2048, 640]               983.68                 0.98\n18_LayerNorm                   [640]          [2048, 640]                 1.28                 0.00\n19_Linear                [640, 1536]         [2048, 1536]               984.58                 0.98\n20_LayerNorm                  [1536]         [2048, 1536]                 3.07                 0.00\n21_Dropout                         -         [2048, 1536]                    -                    -\n22_Linear                [1536, 640]          [2048, 640]               983.68                 0.98\n23_LayerNorm                   [640]          [2048, 640]                 1.28                 0.00\n24_Linear                [640, 1536]         [2048, 1536]               984.58                 0.98\n25_LayerNorm                  [1536]         [2048, 1536]                 3.07                 0.00\n26_Dropout                         -         [2048, 1536]                    -                    -\n27_Linear                [1536, 640]          [2048, 640]               983.68                 0.98\n28_LayerNorm                   [640]          [2048, 640]                 1.28                 0.00\n29_Linear                [640, 1536]         [2048, 1536]               984.58                 0.98\n30_LayerNorm                  [1536]         [2048, 1536]                 3.07                 0.00\n31_Dropout                         -         [2048, 1536]                    -                    -\n32_Linear               [1536, 1024]         [2048, 1024]             1,573.89                 1.57\n33_GELU                            -         [2048, 1024]                    -                    -\n34_LayerNorm                  [1024]         [2048, 1024]                 2.05                 0.00\n35_Linear                [1024, 512]          [2048, 512]               524.80                 0.52\n36_GELU                            -          [2048, 512]                    -                    -\n37_LayerNorm                   [512]          [2048, 512]                 1.02                 0.00\n38_Linear                  [512, 42]           [2048, 42]                21.55                 0.02\n====================================================================================================\n# Params:    16,759.08K\n# Mult-Adds: 16.73M\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# Define Model, Loss Function and Optimizer","metadata":{"id":"uSk4YP63TBnD"}},{"cell_type":"markdown","source":"Here we define the model, loss function, optimizer and optionally a learning rate scheduler.","metadata":{"id":"cwdxl4RiTBnD"}},{"cell_type":"code","source":"#criterion = torch.nn.CrossEntropyLoss() # Defining Loss function.\n# We use CE because the task is multi-class classification\nclass ConservativeSmartLoss(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.num_classes = num_classes\n        self.smoothing = 0.1  # Fixed, reasonable value\n        \n    def forward(self, logits, targets):\n        # Simple label smoothing - no adaptive complications\n        if self.smoothing > 0:\n            smooth_targets = torch.zeros_like(logits).scatter_(\n                1, targets.unsqueeze(1), 1.0 - self.smoothing)\n            smooth_targets += self.smoothing / max(1, self.num_classes - 1)\n            \n            log_probs = F.log_softmax(logits, dim=-1)\n            loss = -torch.sum(smooth_targets * log_probs, dim=-1)\n        else:\n            loss = F.cross_entropy(logits, targets, reduction='none')\n        \n        return loss.mean()\n\ncriterion = ConservativeSmartLoss(num_classes=len(train_data.phonemes)).to(device)\n\n\n# Choose an appropriate optimizer of your choice\noptimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n\n# Recommended : Define Scheduler for Learning Rate,\n# including but not limited to StepLR, MultiStep, CosineAnnealing, CosineAnnealingWithWarmRestarts, ReduceLROnPlateau, etc.\n# You can refer to Pytorch documentation for more information on how to use them.\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer,\n    T_max=config['epochs'],\n    eta_min=1e-6\n)\n\n# Is your training time very high?\n# Look into mixed precision training if your GPU (Tesla T4, V100, etc) can make use of it\n# Refer - https://pytorch.org/docs/stable/notes/amp_examples.html\n# Mixed Precision Training with AMP for speedup\nscaler = torch.amp.GradScaler('cuda', enabled=True)","metadata":{"id":"kZgQ7AgyTBnE","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:21:40.193119Z","iopub.execute_input":"2026-02-02T07:21:40.193947Z","iopub.status.idle":"2026-02-02T07:21:44.395964Z","shell.execute_reply.started":"2026-02-02T07:21:40.193910Z","shell.execute_reply":"2026-02-02T07:21:44.395380Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# Training and Validation Functions","metadata":{"id":"IBwunYpyugFg"}},{"cell_type":"markdown","source":"This section covers the training, and validation functions for each epoch of running your experiment with a given model architecture. The code has been provided to you, but we recommend going through the comments to understand the workflow to enable you to write these loops for future HWs.","metadata":{"id":"1JgeNhx4x2-P"}},{"cell_type":"code","source":"# CLEAR RAM!!\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"id":"XblOHEVtKab2","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:21:44.397117Z","iopub.execute_input":"2026-02-02T07:21:44.397546Z","iopub.status.idle":"2026-02-02T07:21:44.570937Z","shell.execute_reply.started":"2026-02-02T07:21:44.397521Z","shell.execute_reply":"2026-02-02T07:21:44.570222Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"10985"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"def train(model, dataloader, optimizer, criterion):\n\n    model.train()\n    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n\n    for i, (frames, phonemes) in enumerate(dataloader):\n\n        ### Initialize Gradients\n        optimizer.zero_grad()\n\n        frames      = frames.to(device)\n        phonemes    = phonemes.to(device)\n\n        with torch.autocast(device_type=device, dtype=torch.float16):\n            ### Forward Propagation\n            logits  = model(frames)\n\n            ### Loss Calculation\n            loss    = criterion(logits, phonemes)\n\n        ### Backward Propagation\n        scaler.scale(loss).backward()\n\n        # OPTIONAL: You can add gradient clipping here, if you face issues of exploding gradients\n\n        ### Gradient Descent\n        scaler.step(optimizer)\n        scaler.update()\n\n        tloss   += loss.item()\n        tacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n\n        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n                              acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n        batch_bar.update()\n\n        ### Release memory\n        del frames, phonemes, logits\n        torch.cuda.empty_cache()\n\n\n    batch_bar.close()\n    tloss   /= len(train_loader)\n    tacc    /= len(train_loader)\n\n\n    return tloss, tacc","metadata":{"id":"8wjPz7DHqKcL","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:21:49.665494Z","iopub.execute_input":"2026-02-02T07:21:49.665838Z","iopub.status.idle":"2026-02-02T07:21:49.673320Z","shell.execute_reply.started":"2026-02-02T07:21:49.665810Z","shell.execute_reply":"2026-02-02T07:21:49.672289Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Combine train and val datasets for cross-validation\ncombined_data = ConcatDataset([train_data, val_data])\nprint(f\"Combined dataset size: {len(combined_data)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:21:51.506837Z","iopub.execute_input":"2026-02-02T07:21:51.507363Z","iopub.status.idle":"2026-02-02T07:21:51.511638Z","shell.execute_reply.started":"2026-02-02T07:21:51.507332Z","shell.execute_reply":"2026-02-02T07:21:51.510930Z"}},"outputs":[{"name":"stdout","text":"Combined dataset size: 38019361 samples\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Define the input size\nINPUT_SIZE = (2*config['context'] + 1) * 28","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:21:55.169650Z","iopub.execute_input":"2026-02-02T07:21:55.169985Z","iopub.status.idle":"2026-02-02T07:21:55.173799Z","shell.execute_reply.started":"2026-02-02T07:21:55.169960Z","shell.execute_reply":"2026-02-02T07:21:55.173021Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def train_with_cv(combined_data, num_folds=3, use_wandb=True):\n    \"\"\"\n    Perform k-fold cross-validation on combined dataset\n    \"\"\"\n    total_samples = len(combined_data)\n    indices = np.arange(total_samples)\n    np.random.shuffle(indices)\n    \n    fold_size = total_samples // num_folds\n    fold_results = []\n    fold_models = []\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Starting {num_folds}-Fold Cross-Validation\")\n    print(f\"Total samples: {total_samples}, Fold size: ~{fold_size}\")\n    print(f\"{'='*60}\")\n    \n    for fold in range(num_folds):\n        print(f\"\\n{'='*60}\")\n        print(f\"Training Fold {fold + 1}/{num_folds}\")\n        print(f\"{'='*60}\")\n        \n        # Create validation indices for this fold\n        val_start = fold * fold_size\n        val_end = (fold + 1) * fold_size if fold < num_folds - 1 else total_samples\n        val_indices = indices[val_start:val_end]\n        train_indices = np.concatenate([indices[:val_start], indices[val_end:]])\n        \n        print(f\"Train samples: {len(train_indices)}, Val samples: {len(val_indices)}\")\n        \n        # Create subset datasets\n        train_subset = Subset(combined_data, train_indices)\n        val_subset = Subset(combined_data, val_indices)\n        \n        # Create dataloaders for this fold\n        # Note: We need to use the collate_fn from the original dataset\n        train_loader = torch.utils.data.DataLoader(\n            dataset=train_subset,\n            num_workers=4,\n            batch_size=config['batch_size'],\n            pin_memory=True,\n            shuffle=True,\n            collate_fn=train_data.collate_fn  # Use original train_data's collate_fn\n        )\n        \n        val_loader = torch.utils.data.DataLoader(\n            dataset=val_subset,\n            num_workers=0,\n            batch_size=config['batch_size'],\n            pin_memory=True,\n            shuffle=False,\n            collate_fn=train_data.collate_fn  # Use original train_data's collate_fn\n        )\n        \n        # Initialize fresh model for this fold\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n        model = Network(INPUT_SIZE, len(train_data.phonemes)).to(device)\n        \n        # Check parameter count\n        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        print(f\"Model parameters: {total_params:,}\")\n        assert total_params < 20_000_000, \"Exceeds 20 MIL params!\"\n        \n        # Initialize optimizer and scheduler\n        optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n        \n        # Using Cosine Annealing for better convergence\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n                    optimizer,\n                    T_max=config['epochs'],\n                    eta_min=1e-6\n                )\n        \n        # Initialize gradient scaler for mixed precision\n        scaler = torch.cuda.amp.GradScaler()\n        \n        # Training loop for this fold\n        best_val_acc = 0\n        best_epoch = 0\n        patience_counter = 0\n        early_stop_patience = config.get('early_stop_patience', 10)\n        \n        for epoch in range(config['epochs']):\n            print(f\"\\nFold {fold+1} - Epoch {epoch+1}/{config['epochs']}\")\n            \n            # Train\n            model.train()\n            train_loss, train_acc = 0, 0\n            batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n            \n            for i, (frames, phonemes) in enumerate(train_loader):\n                optimizer.zero_grad()\n                frames = frames.to(device)\n                phonemes = phonemes.to(device)\n                \n                with torch.autocast(device_type=device, dtype=torch.float16):\n                    logits = model(frames)\n                    loss = criterion(logits, phonemes)\n                \n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n                \n                train_loss += loss.item()\n                train_acc += torch.sum(torch.argmax(logits, dim=1) == phonemes).item() / len(phonemes)\n                \n                batch_bar.set_postfix(\n                    loss=f\"{float(train_loss / (i + 1)):.04f}\",\n                    acc=f\"{float(train_acc * 100 / (i + 1)):.02f}%\"\n                )\n                batch_bar.update()\n                \n                del frames, phonemes, logits\n                torch.cuda.empty_cache()\n            \n            batch_bar.close()\n            train_loss /= len(train_loader)\n            train_acc /= len(train_loader)\n            \n            # Validate\n            model.eval()\n            val_loss, val_acc = 0, 0\n            batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, leave=False, position=0, desc='Val')\n            \n            with torch.inference_mode():\n                for i, (frames, phonemes) in enumerate(val_loader):\n                    frames = frames.to(device)\n                    phonemes = phonemes.to(device)\n                    \n                    logits = model(frames)\n                    loss = criterion(logits, phonemes)\n                    \n                    val_loss += loss.item()\n                    val_acc += torch.sum(torch.argmax(logits, dim=1) == phonemes).item() / len(phonemes)\n                    \n                    batch_bar.set_postfix(\n                        loss=f\"{float(val_loss / (i + 1)):.04f}\",\n                        acc=f\"{float(val_acc * 100 / (i + 1)):.02f}%\"\n                    )\n                    batch_bar.update()\n                    \n                    del frames, phonemes, logits\n                    torch.cuda.empty_cache()\n            \n            batch_bar.close()\n            val_loss /= len(val_loader)\n            val_acc /= len(val_loader)\n            \n            # Log metrics\n            curr_lr = float(optimizer.param_groups[0]['lr'])\n            print(f\"\\tTrain Acc: {train_acc*100:.2f}%, Loss: {train_loss:.4f}\")\n            print(f\"\\tVal Acc: {val_acc*100:.2f}%, Loss: {val_loss:.4f}, LR: {curr_lr:.7f}\")\n            \n            if use_wandb:\n                wandb.log({\n                    f'fold_{fold}_train_acc': train_acc * 100,\n                    f'fold_{fold}_train_loss': train_loss,\n                    f'fold_{fold}_val_acc': val_acc * 100,\n                    f'fold_{fold}_val_loss': val_loss,\n                    f'fold_{fold}_lr': curr_lr,\n                    'epoch': epoch\n                })\n            \n            # Save best model for this fold\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n                best_epoch = epoch\n                patience_counter = 0\n                \n                # Save model checkpoint\n                checkpoint = {\n                    'fold': fold,\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'val_acc': val_acc,\n                    'train_acc': train_acc,\n                    'config': config\n                }\n                torch.save(checkpoint, f'fold_{fold}_best_model.pth')\n                print(f\"\\t✓ New best model saved! Val Acc: {val_acc*100:.2f}%\")\n            else:\n                patience_counter += 1\n                if patience_counter >= early_stop_patience:\n                    print(f\"\\t⚠ Early stopping triggered after {patience_counter} epochs without improvement\")\n                    break\n            \n            # Step scheduler\n            scheduler.step()\n        \n        # Store fold results\n        fold_results.append(best_val_acc)\n        fold_models.append(model)  # Store the trained model\n        \n        print(f\"\\nFold {fold+1} completed. Best Val Acc: {best_val_acc*100:.2f}% at epoch {best_epoch+1}\")\n        \n        # Clean up for next fold\n        del model, optimizer, scheduler, scaler\n        torch.cuda.empty_cache()\n        gc.collect()\n    \n    # Print cross-validation summary\n    print(f\"\\n{'='*60}\")\n    print(f\"{num_folds}-Fold Cross-Validation Results:\")\n    print(f\"{'='*60}\")\n    fold_accs = [acc * 100 for acc in fold_results]\n    for i, acc in enumerate(fold_accs):\n        print(f\"Fold {i+1}: {acc:.2f}%\")\n    \n    print(f\"\\nMean CV Accuracy: {np.mean(fold_accs):.2f}%\")\n    print(f\"Std CV Accuracy: {np.std(fold_accs):.2f}%\")\n    print(f\"Min CV Accuracy: {np.min(fold_accs):.2f}%\")\n    print(f\"Max CV Accuracy: {np.max(fold_accs):.2f}%\")\n    \n    return fold_results, fold_models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:21:59.865820Z","iopub.execute_input":"2026-02-02T07:21:59.866153Z","iopub.status.idle":"2026-02-02T07:21:59.888024Z","shell.execute_reply.started":"2026-02-02T07:21:59.866125Z","shell.execute_reply":"2026-02-02T07:21:59.887381Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def eval(model, dataloader):\n\n    model.eval() # set model in evaluation mode\n    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n\n    for i, (frames, phonemes) in enumerate(dataloader):\n\n        ### Move data to device (ideally GPU)\n        frames      = frames.to(device)\n        phonemes    = phonemes.to(device)\n\n        # makes sure that there are no gradients computed as we are not training the model now\n        with torch.inference_mode():\n            ### Forward Propagation\n            logits  = model(frames)\n            ### Loss Calculation\n            loss    = criterion(logits, phonemes)\n\n        vloss   += loss.item()\n        vacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n\n        # Do you think we need loss.backward() and optimizer.step() here?\n\n        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))),\n                              acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1))))\n        batch_bar.update()\n\n        ### Release memory\n        del frames, phonemes, logits\n        torch.cuda.empty_cache()\n\n    batch_bar.close()\n    vloss   /= len(val_loader)\n    vacc    /= len(val_loader)\n\n    return vloss, vacc","metadata":{"id":"Q5npQNFH315V","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:22:05.419491Z","iopub.execute_input":"2026-02-02T07:22:05.419839Z","iopub.status.idle":"2026-02-02T07:22:05.426741Z","shell.execute_reply.started":"2026-02-02T07:22:05.419811Z","shell.execute_reply":"2026-02-02T07:22:05.425900Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# Weights and Biases Setup","metadata":{"id":"yMd_XxPku5qp"}},{"cell_type":"markdown","source":"This section is to enable logging metrics and files with Weights and Biases. Please refer to wandb documentationa and recitation 0 that covers the use of weights and biases for logging, hyperparameter tuning and monitoring your runs for your homeworks. Using this tool makes it very easy to show results when submitting your code and models for homeworks, and also extremely useful for study groups to organize and run ablations under a single team in wandb.\n\nWe have written code for you to make use of it out of the box, so that you start using wandb for all your HWs from the beginning.","metadata":{"id":"tjIbhR1wwbgI"}},{"cell_type":"code","source":"wandb.login(key=\"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\") #API Key is in your wandb account, under settings (wandb.ai/settings)","metadata":{"id":"SCDYx5VEu6qI","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:22:10.814963Z","iopub.execute_input":"2026-02-02T07:22:10.815268Z","iopub.status.idle":"2026-02-02T07:22:18.136099Z","shell.execute_reply.started":"2026-02-02T07:22:10.815241Z","shell.execute_reply":"2026-02-02T07:22:18.135515Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n  | |_| | '_ \\/ _` / _` |  _/ -_)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Using explicit session credentials for https://api.wandb.ai.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnyatuka2407\u001b[0m (\u001b[33mnyatuka2407-multimdia-university-of-kenya\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# Create your wandb run\nRESUME_OLD_RUN = False\n\nif RESUME_OLD_RUN == True:\n    print(\"Resuming previous WanDB run...\")\n    run = wandb.init(\n        name    = \"first-run1\", ### Wandb creates random run names if you skip this field, we recommend you give useful names\n        id     = \"\", ### Insert specific run id here if you want to resume a previous run\n        resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n        project = \"hw1p2\", ### Project should be created in your wandb account\n        config  = config ### Wandb Config for your run\n    )\nelse:\n    print(\"Initializing new WanDB run...\")\n    run = wandb.init(\n        name    = \"first-run1\", ### Wandb creates random run names if you skip this field, we recommend you give useful names\n        reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n        project = \"hw1p2\", ### Project should be created in your wandb account\n        config  = config ### Wandb Config for your run\n    )","metadata":{"id":"xvUnYd3Bw2up","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:22:18.137162Z","iopub.execute_input":"2026-02-02T07:22:18.137554Z","iopub.status.idle":"2026-02-02T07:22:25.698094Z","shell.execute_reply.started":"2026-02-02T07:22:18.137529Z","shell.execute_reply":"2026-02-02T07:22:25.697526Z"}},"outputs":[{"name":"stdout","text":"Initializing new WanDB run...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.24.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20260202_072218-mpyne0j5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nyatuka2407-multimdia-university-of-kenya/hw1p2/runs/mpyne0j5' target=\"_blank\">first-run1</a></strong> to <a href='https://wandb.ai/nyatuka2407-multimdia-university-of-kenya/hw1p2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nyatuka2407-multimdia-university-of-kenya/hw1p2' target=\"_blank\">https://wandb.ai/nyatuka2407-multimdia-university-of-kenya/hw1p2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nyatuka2407-multimdia-university-of-kenya/hw1p2/runs/mpyne0j5' target=\"_blank\">https://wandb.ai/nyatuka2407-multimdia-university-of-kenya/hw1p2/runs/mpyne0j5</a>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"### Save your model architecture as a string with str(model)\nmodel_arch  = str(model)\n\n### Save it in a txt file\narch_file   = open(\"model_arch.txt\", \"w\")\nfile_write  = arch_file.write(model_arch)\narch_file.close()\n\n### log it in your wandb run with wandb.save()\nwandb.save('model_arch.txt')","metadata":{"id":"wft15E_IxYFi","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:22:50.488596Z","iopub.execute_input":"2026-02-02T07:22:50.489311Z","iopub.status.idle":"2026-02-02T07:22:50.499986Z","shell.execute_reply.started":"2026-02-02T07:22:50.489279Z","shell.execute_reply":"2026-02-02T07:22:50.499383Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 1 file into the W&B run directory; call wandb.save again to sync new files.\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/wandb/run-20260202_072218-mpyne0j5/files/model_arch.txt']"},"metadata":{}}],"execution_count":29},{"cell_type":"markdown","source":"# Experiment","metadata":{"id":"nclx_04fu7Dd"}},{"cell_type":"markdown","source":"Now, it is time to finally run your ablations! Have fun!","metadata":{"id":"MdLMWfEpyGOB"}},{"cell_type":"code","source":"'''\n# Iterate over number of epochs to train and evaluate your model\ntorch.cuda.empty_cache()\ngc.collect()\n#wandb.watch(model, log=\"all\")\n\nfor epoch in range(config['epochs']):\n\n    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n\n    curr_lr                 = float(optimizer.param_groups[0]['lr'])\n    train_loss, train_acc   = train(model, train_loader, optimizer, criterion)\n    val_loss, val_acc       = eval(model, val_loader)\n\n    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\n    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\n\n    ## Log metrics at each epoch in your run\n    # Optionally, you can log at each batch inside train/eval functions\n    # (explore wandb documentation/wandb recitation)\n    wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss,\n               'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})\n\n    # If using a scheduler, step the learning rate here, otherwise comment this line\n    # Depending on the scheduler in use, you may or may not need to pass in a metric into the step function, so read the docs well\n    scheduler.step(val_acc)\n\n    ## HIGHLY RECOMMENDED: Save model checkpoint in drive and/or wandb if accuracy is better than your current best accuracy\n    ## This enables you to resume training at anytime, without having to start from scratch.\n    ## Refer to Recitation 0.24 to learn how to implement this: https://www.youtube.com/watch?v=-TCH0DbUEKM&list=PLp-0K3kfddPw2D5CeA09lsx_oNy9E\n    \n'''","metadata":{"id":"4NNCA5DDTBnO","trusted":true,"execution":{"iopub.status.busy":"2026-02-01T11:32:55.390059Z","iopub.execute_input":"2026-02-01T11:32:55.390311Z","iopub.status.idle":"2026-02-01T11:32:55.400151Z","shell.execute_reply.started":"2026-02-01T11:32:55.390261Z","shell.execute_reply":"2026-02-01T11:32:55.399590Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"'\\n# Iterate over number of epochs to train and evaluate your model\\ntorch.cuda.empty_cache()\\ngc.collect()\\n#wandb.watch(model, log=\"all\")\\n\\nfor epoch in range(config[\\'epochs\\']):\\n\\n    print(\"\\nEpoch {}/{}\".format(epoch+1, config[\\'epochs\\']))\\n\\n    curr_lr                 = float(optimizer.param_groups[0][\\'lr\\'])\\n    train_loss, train_acc   = train(model, train_loader, optimizer, criterion)\\n    val_loss, val_acc       = eval(model, val_loader)\\n\\n    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\\n    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\\n\\n    ## Log metrics at each epoch in your run\\n    # Optionally, you can log at each batch inside train/eval functions\\n    # (explore wandb documentation/wandb recitation)\\n    wandb.log({\\'train_acc\\': train_acc*100, \\'train_loss\\': train_loss,\\n               \\'val_acc\\': val_acc*100, \\'valid_loss\\': val_loss, \\'lr\\': curr_lr})\\n\\n    # If using a scheduler, step the learning rate here, otherwise comment this line\\n    # Depending on the scheduler in use, you may or may not need to pass in a metric into the step function, so read the docs well\\n    scheduler.step(val_acc)\\n\\n    ## HIGHLY RECOMMENDED: Save model checkpoint in drive and/or wandb if accuracy is better than your current best accuracy\\n    ## This enables you to resume training at anytime, without having to start from scratch.\\n    ## Refer to Recitation 0.24 to learn how to implement this: https://www.youtube.com/watch?v=-TCH0DbUEKM&list=PLp-0K3kfddPw2D5CeA09lsx_oNy9E\\n    \\n'"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"# Run cross-validation\nfold_results, fold_models = train_with_cv(\n    combined_data,\n    use_wandb=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T11:32:55.401879Z","iopub.execute_input":"2026-02-01T11:32:55.402438Z","execution_failed":"2026-02-01T12:14:00.449Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nStarting 3-Fold Cross-Validation\nTotal samples: 38019361, Fold size: ~12673120\n============================================================\n\n============================================================\nTraining Fold 1/3\n============================================================\nTrain samples: 25346241, Val samples: 12673120\nModel parameters: 16,759,082\n\nFold 1 - Epoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/3281246084.py:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/12377 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val:   0%|          | 0/6189 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Acc: 70.34%, Loss: 1.4859\n\tVal Acc: 76.90%, Loss: 1.2889, LR: 0.0005000\n\t✓ New best model saved! Val Acc: 76.90%\n\nFold 1 - Epoch 2/25\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/12377 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bd24bfeaf7d44b1be17cbd44ac3bde3"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"\n# Initialize empty list for fold models\nfold_models = []\n\n# List of your saved model files\nsaved_model_files = [\n    '/kaggle/input/best-run/fold_0_best_model.pth',\n    '/kaggle/input/best-run/fold_1_best_model.pth', \n]\n\nprint(\"Loading saved fold models...\")\nprint(\"-\" * 50)\n\n# Load each model file\nfor i, model_file in enumerate(saved_model_files):\n    try:\n        # Load the checkpoint\n        checkpoint = torch.load(model_file, map_location='cpu')\n        \n        # Create a new model instance with the same architecture\n        model = Network(INPUT_SIZE, len(train_data.phonemes))\n        \n        # Check what's in the checkpoint\n        print(f\"\\n{model_file} contains:\")\n        for key in checkpoint.keys():\n            print(f\"  - {key}: {type(checkpoint[key]).__name__}\")\n        \n        # Load the model weights\n        if 'model_state_dict' in checkpoint:\n            model.load_state_dict(checkpoint['model_state_dict'])\n            print(f\"✓ Loaded model weights from 'model_state_dict'\")\n            if 'val_acc' in checkpoint:\n                print(f\"  Validation accuracy: {checkpoint['val_acc']*100:.2f}%\")\n        else:\n            # If the checkpoint is just the model state dict directly\n            model.load_state_dict(checkpoint)\n            print(f\"✓ Loaded model weights (direct state dict)\")\n        \n        # Move model to GPU if available\n        model = model.to(device)\n        \n        # Set model to evaluation mode\n        model.eval()\n        \n        # Add to fold_models list\n        fold_models.append(model)\n        \n        print(f\"✓ Successfully loaded {model_file} as fold_models[{i}]\")\n        \n    except FileNotFoundError:\n        print(f\"✗ ERROR: {model_file} not found!\")\n        print(\"Make sure the file is in your current working directory.\")\n    except Exception as e:\n        print(f\"✗ ERROR loading {model_file}: {str(e)}\")\n        print(\"Check if the model architecture matches your Network class.\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(f\"Successfully loaded {len(fold_models)} models into fold_models\")\nprint(\"=\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:23:16.344395Z","iopub.execute_input":"2026-02-02T07:23:16.344705Z","iopub.status.idle":"2026-02-02T07:23:19.629955Z","shell.execute_reply.started":"2026-02-02T07:23:16.344680Z","shell.execute_reply":"2026-02-02T07:23:19.629241Z"}},"outputs":[{"name":"stdout","text":"Loading saved fold models...\n--------------------------------------------------\n\n/kaggle/input/best-run/fold_0_best_model.pth contains:\n  - fold: int\n  - epoch: int\n  - model_state_dict: OrderedDict\n  - optimizer_state_dict: dict\n  - val_acc: float\n  - train_acc: float\n  - config: dict\n✓ Loaded model weights from 'model_state_dict'\n  Validation accuracy: 88.77%\n✓ Successfully loaded /kaggle/input/best-run/fold_0_best_model.pth as fold_models[0]\n\n/kaggle/input/best-run/fold_1_best_model.pth contains:\n  - fold: int\n  - epoch: int\n  - model_state_dict: OrderedDict\n  - optimizer_state_dict: dict\n  - val_acc: float\n  - train_acc: float\n  - config: dict\n✓ Loaded model weights from 'model_state_dict'\n  Validation accuracy: 88.75%\n✓ Successfully loaded /kaggle/input/best-run/fold_1_best_model.pth as fold_models[1]\n\n==================================================\nSuccessfully loaded 2 models into fold_models\n==================================================\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# Testing and submission to Kaggle","metadata":{"id":"_kXwf5YUo_4A"}},{"cell_type":"markdown","source":"Before we get to the following code, make sure to see the format of submission given in *sample_submission.csv*. Once you have done so, it is time to fill the following function to complete your inference on test data. Refer the eval function from previous cells to get an idea of how to go about completing this function.","metadata":{"id":"WI1hSFYLpJvH"}},{"cell_type":"code","source":"phoneme2idx = {p: i for i, p in enumerate(PHONEMES)}\nidx2phoneme = {i: p for i, p in enumerate(PHONEMES)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:23:24.300313Z","iopub.execute_input":"2026-02-02T07:23:24.300605Z","iopub.status.idle":"2026-02-02T07:23:24.306120Z","shell.execute_reply.started":"2026-02-02T07:23:24.300583Z","shell.execute_reply":"2026-02-02T07:23:24.305530Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def test(model, test_loader):\n    ### What you call for model to perform inference?\n    model.eval() # TODO train or eval?\n\n    ### List to store predicted phonemes of test data\n    test_predictions = []\n\n    ### Which mode do you need to avoid gradients?\n    with torch.no_grad(): # TODO\n\n        for i, mfccs in enumerate(tqdm(test_loader)):\n\n            mfccs   = mfccs.to(device)\n\n            logits  = model(mfccs)\n\n            ### Get most likely predicted phoneme with argmax\n            predicted_phonemes = torch.argmax(logits, dim=1)\n            predicted_phonemes = predicted_phonemes.cpu().tolist()\n\n            ### How do you store predicted_phonemes with test_predictions? HINT: look at the eval() function from earlier\n            # Remember the phonemes were converted from strings to their corresponding integer indices earlier, and the results of the argmax is a list of the integer indices of the predicted phonemes.\n            # So how do you get and store the actual predicted phonemes (strings NOT integers)\n            # TODO: Convert predicted_phonemes (integer indices from argmax) back to phoneme strings and append them to test_predictions\n            for idx in predicted_phonemes:\n                phoneme_str = idx2phoneme[idx]\n                test_predictions.append(phoneme_str)\n\n    ## SANITY CHECK\n    sample_predictions = test_predictions[:10]\n    if not isinstance(sample_predictions[0], str):\n        print(f\"❌ ERROR: Predictions should be phoneme STRINGS, not {type(sample_predictions[0]).__name__}!\")\n        print(f\"   You need to convert integer indices to their corresponding phoneme strings\")\n        print(f\"   Hint: Look at the eval() function to get the idea\")\n\n    # Print a preview of predictions for manual inspection\n    print(\"\\nSample predictions:\", sample_predictions)\n    print(\"\\nPredictions Generated successfully!\")\n\n    return test_predictions","metadata":{"id":"ijLrIJFl5dSf","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:23:27.402099Z","iopub.execute_input":"2026-02-02T07:23:27.402808Z","iopub.status.idle":"2026-02-02T07:23:27.410288Z","shell.execute_reply.started":"2026-02-02T07:23:27.402777Z","shell.execute_reply":"2026-02-02T07:23:27.409687Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def create_ensemble_predictions(fold_models, test_dataset):\n    print(\"\\nGenerating ensemble predictions (mean-logits)...\")\n\n    ensemble_predictions = []\n\n    with torch.no_grad():\n        for seq_idx in tqdm(range(len(test_dataset))):\n            frames = test_dataset[seq_idx]   # (T, 41, 28)\n\n            for t in range(frames.shape[0]):\n                input_vector = frames[t].reshape(1, -1).to(device)  # (1, 1148)\n\n                logits_list = []\n                for model in fold_models:\n                    logits = model(input_vector)\n                    logits_list.append(logits)\n\n                avg_logits = torch.mean(torch.stack(logits_list), dim=0)\n                pred_idx = torch.argmax(avg_logits, dim=1).item()\n                ensemble_predictions.append(pred_idx)\n\n    ensemble_predictions = [idx2phoneme[idx] for idx in ensemble_predictions]\n    return ensemble_predictions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:23:30.493295Z","iopub.execute_input":"2026-02-02T07:23:30.493596Z","iopub.status.idle":"2026-02-02T07:23:30.500538Z","shell.execute_reply.started":"2026-02-02T07:23:30.493571Z","shell.execute_reply":"2026-02-02T07:23:30.500033Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# Generate model test predictions\n\n#predictions = test(model, test_loader)","metadata":{"id":"wG9v6Xmxu7wp","trusted":true,"execution":{"execution_failed":"2026-02-01T12:14:00.452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#predictions = create_ensemble_predictions(fold_models, test_loader)\n\npredictions = create_ensemble_predictions(fold_models, test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:23:33.663894Z","iopub.execute_input":"2026-02-02T07:23:33.664482Z","iopub.status.idle":"2026-02-02T08:43:54.265017Z","shell.execute_reply.started":"2026-02-02T07:23:33.664452Z","shell.execute_reply":"2026-02-02T08:43:54.264170Z"}},"outputs":[{"name":"stdout","text":"\nGenerating ensemble predictions (mean-logits)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2620 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a79e24ecb5834d87b6953944d382da08"}},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"### Create CSV file with predictions\n\nwith open(\"./submission.csv\", \"w+\") as f:\n    f.write(\"id,label\\n\")\n    for i in range(len(predictions)):\n        f.write(\"{},{}\\n\".format(i, predictions[i]))\n\n    print(\"submission.csv file created successfully!\")","metadata":{"id":"_I6AVEY45dSg","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T08:47:18.596475Z","iopub.execute_input":"2026-02-02T08:47:18.596895Z","iopub.status.idle":"2026-02-02T08:47:19.702295Z","shell.execute_reply.started":"2026-02-02T08:47:18.596862Z","shell.execute_reply":"2026-02-02T08:47:19.701664Z"}},"outputs":[{"name":"stdout","text":"submission.csv file created successfully!\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"### Finish your wandb run\nrun.finish()","metadata":{"id":"6Wf-P25TXU0N","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T08:47:22.869449Z","iopub.execute_input":"2026-02-02T08:47:22.869765Z","iopub.status.idle":"2026-02-02T08:47:23.496386Z","shell.execute_reply.started":"2026-02-02T08:47:22.869739Z","shell.execute_reply":"2026-02-02T08:47:23.495775Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">first-run1</strong> at: <a href='https://wandb.ai/nyatuka2407-multimdia-university-of-kenya/hw1p2/runs/mpyne0j5' target=\"_blank\">https://wandb.ai/nyatuka2407-multimdia-university-of-kenya/hw1p2/runs/mpyne0j5</a><br> View project at: <a href='https://wandb.ai/nyatuka2407-multimdia-university-of-kenya/hw1p2' target=\"_blank\">https://wandb.ai/nyatuka2407-multimdia-university-of-kenya/hw1p2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20260202_072218-mpyne0j5/logs</code>"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"# Submit to kaggle competition using kaggle API","metadata":{"id":"nvbyxVa67ev6"}},{"cell_type":"code","source":"if \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ:\n    # If on kaggle:\n    !kaggle competitions submit -c hw-1-p-2-spring-2026-student-competition -f submission.csv -m \"Test Submission\"\nelse:\n    # If NOT on kaggle:\n    # Adjust path of '/content/submission.csv' if necessary\n    !kaggle competitions submit -c hw-1-p-2-spring-2026-student-competition -f /content/submission.csv -m \"Test Submission\"\n\n### If this fails, you can download the csv file and then submit directly on the kaggle competition page","metadata":{"id":"LjcammuCxMKN","trusted":true,"execution":{"iopub.status.busy":"2026-02-02T08:47:46.953788Z","iopub.execute_input":"2026-02-02T08:47:46.954093Z","iopub.status.idle":"2026-02-02T08:47:49.687965Z","shell.execute_reply.started":"2026-02-02T08:47:46.954068Z","shell.execute_reply":"2026-02-02T08:47:49.687065Z"}},"outputs":[{"name":"stdout","text":"100%|██████████████████████████████████████| 19.3M/19.3M [00:00<00:00, 32.2MB/s]\nSuccessfully submitted to HW1P2_Spring_2026_Student_Competition","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"## TODO: DO NOT MODIFY, RUN AS IS\n#### Generate a model_metadata.json file to save your model's data (due 48 hours after Kaggle submission deadline OR the day of slack submission)","metadata":{"id":"9Zb7jyfV7ev6"}},{"cell_type":"code","source":"import json, os, sys, torch, zipfile, datetime\n################################\n# TODO: Keep the model_metadata.json\n# file safe for submission later.\n################################\ndef generate_model_submission_file():\n    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n    json_filename = f\"model_metadata_{timestamp}.json\"\n\n    # Create JSON with parameter count, model architecture, and predictions\n    output_json = {\n        \"parameter_count\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n        \"model_architecture\": str(model),\n    }\n\n    # Save metadata JSON\n    with open(json_filename, \"w\") as f:\n        json.dump(output_json, f, indent=2)\n\n    # Download / display link depending on environment\n    if \"google.colab\" in sys.modules and \"COLAB_GPU\" in os.environ:\n        from google.colab import files\n        files.download(json_filename)\n    elif \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ:\n        from IPython.display import FileLink, display\n        print(\"#\" * 100)\n        print(f\"Your submission file `{json_filename}` has been generated.\")\n        print(\"TODO: Click the link below.\")\n        print(\"1. The file will open in a new tab.\")\n        print(\"2. Right-click anywhere in the new tab and select 'Save As...'\")\n        print(\"3. Save the file to your computer with the `.json` extension.\")\n        print(\"You MUST submit this file to Autolab if this is your best submission.\")\n        print(\"#\" * 100 + \"\\n\")\n        display(FileLink(json_filename))\n    else:\n        print(f\"✅ saved model data saved to: '{json_filename}'\")\n        print(\"REQUIRED to submit to Autolab if these are the best model weights.\")\n\ngenerate_model_submission_file()","metadata":{"id":"5wHrc5VV7ev6","trusted":true,"execution":{"execution_failed":"2026-02-01T12:14:00.453Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Code Submission Section","metadata":{"id":"QRWJQGmD7ev7"}},{"cell_type":"code","source":"####################################\n#             README\n####################################\n\n# TODO: Please complete all components of this README\nREADME = \"\"\"\n- **Model**: Model archtiecture description. Anything unique? Any specific architecture shapes or strategies?\n- **Training Strategy**: optimizer + scheduler + loss function + any other unique ideas\n- **Augmentations**: augmentations if used. If augmentations weren't used, then ignore\n- **Notebook Execution**: Any instructions required to run your notebook.\n\"\"\"\n\n####################################\n#       Credentials (Optional)\n####################################\n\n# These are not required **IF** you have run the cells to declare these variables above.\n# If you would like to paste your credentials here again, feel free to:\n# OPTIONAL: Fill these out if you do not want to re-run previous cells to re-initialize these credential variables\n\nif \"KAGGLE_USERNAME\" not in globals():\n  KAGGLE_USERNAME = None # Optional: Put your kaggle username here\n\nif \"KAGGLE_API_KEY\" not in globals():\n  KAGGLE_API_KEY = None # Optional: Put your kaggle api key here\n\nif \"WANDB_API_KEY\" not in globals():\n  WANDB_API_KEY = None # Optional: Put your wandb api key here\n\n\n####################################\n#             Wandb Logs\n####################################\n\n# TODO: Your wandb project url should look like https://wandb.ai/username-or-team-name/project-name\n#(Take these parameters and put them in the variables below)\n\nWANDB_USERNAME_OR_TEAMNAME = \"YOUR USER NAME\" # TODO: Put your username-or-team-name here\nWANDB_PROJECT = \"hw1p2\" # TODO: Put your project-name\n\n####################################\n#         Notebook & Files\n####################################\n\n# TODO: Download HW1P2 Notebook (if on colab or kaggle) and upload both your HW1P2 notebook + model_metadata_*.json to your file system.\n# TODO: For each file, obtain the file paths and put them below.\n\n# TODO: COLAB INSTRUCTIONS:\n# * With Colab, upload your desired file (notebook or model_metadata.json) to \"Files\"\n# * Right-click the file, click \"Copy Path,\"\n# * Paste the path below.\n\n# TODO: KAGGLE INSTRUCTIONS:\n# * First download a copy of your notebook with \"File > Download Notebook\"\n# Then...\n# * Click \"File\" in the top left of the screen\n# * Go to \"Upload Input > Upload Model\"\n# * Upload your notebook file.\n# * For \"Model Name\" put HW1P2_Final_Submission\n# * For \"Framework\" put \"Other\"\n# * For \"License\" put \"Other\"\n# * Click \"Upload another file\" and upload your model_metadata####.json file as well.\n# * Now, on your right in your \"Models\" section, you should see a new folder with your submission files.\n# * Click on the \"Copy File Path\" buttons for the notebook and json file and paste them below.\n\n# TODO: Linux system:\n# * Simply upload or find the path of your notebook file and model_metadata###.json file, and paste them here.\n\nNOTEBOOK_PATH = \"/content/<YOUR_NOTEBOOK_PATH>.ipynb\" # TODO: Put your HW1P2 notebook path here\nMODEL_METADATA_JSON = \"/content/<YOUR_METADATA_JSON_PATH>.json\" # TODO: Put your Model Metadata path json file here (see end of HW1P2 Code Notebook to get this file)\n\n\n####################################\n#         Additional Files\n####################################\n\nADDITIONAL_FILES = [ # TODO: Upload any files and add any paths to any additional files you would like to include in your submission, otherwise, leave this empty\n]\n\n####################################\n#         SLACK SUBMISSION\n####################################\n\nENABLE_SLACK_SUBMISSION = False # TODO: Set this to true if you are submitting to the Slack competition\n\n####################################\n#     Creating the Submission\n####################################\n\n# TODO: Once the README, wandb information, and file paths are filled in, run this cell,\n# run the \"Assignment Backend Functions\" in the next cells, and generate the final zip file at the end.\n\nSAFE_SUBMISSION = True # TODO: Set this to False if you want to generate a submission.zip even if you are missing files, otherwise it's recommended to keep this as True\n","metadata":{"id":"qo31VIEM7ev7","trusted":true,"execution":{"execution_failed":"2026-02-01T12:14:00.453Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Assignment Backend Submission Functions (DO NOT MODIFY, just run these cells)","metadata":{"id":"WBuvtTWS7ev7"}},{"cell_type":"code","source":"from datetime import datetime\n\n######################################\n#       Assignment Configs\n######################################\n\nWANDB_METRIC = \"val_acc\"\nWANDB_DIRECTION = \"ascending\"\nWANDB_TOP_N = 10\nWANDB_OUTPUT_PKL = \"wandb_top_runs.pkl\"\n\n# Kaggle configuration\nCOMPETITION_NAME = \"hw-1-p-2-spring-2026-student-competition\"\nSLACK_COMPETITION_NAME = \"hw-1-p-2-spring-2026-student-slack-submission\"\nFINAL_SUBMISSION_DATETIME = datetime.strptime(\"2026-02-06 23:59:59\", \"%Y-%m-%d %H:%M:%S\")\nSLACK_SUBMISSION_DATETIME = datetime.strptime(\"2026-02-13 23:59:59\", \"%Y-%m-%d %H:%M:%S\")\nGRADING_DIRECTION = \"ascending\"\nKAGGLE_OUTPUT_JSON = \"kaggle_data.json\"\n\nSUBMISSION_OUTPUT = \"HW1P2_final_submission.zip\"\n","metadata":{"id":"gsz5we7A7ev7","trusted":true,"execution":{"execution_failed":"2026-02-01T12:14:00.453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def strict_wandb_extract(obj):\n    \"\"\"\n    Robustly extracts data from WandB objects, handling strings,\n    broken wrappers, and standard dicts.\n    \"\"\"\n    # 1. If it's already a string, try to parse it as JSON.\n    # If not JSON, return the string as-is.\n    if isinstance(obj, str):\n        try:\n            return json.loads(obj)\n        except (ValueError, TypeError):\n            return obj\n\n    # 2. Workaround for the library crash:\n    # If it has the internal '_json_dict' attribute, use that directly\n    # to bypass the broken .items() method.\n    if hasattr(obj, '_json_dict'):\n        return strict_wandb_extract(obj._json_dict)\n\n    # 3. If it behaves like a normal dict, use .items()\n    if hasattr(obj, 'items'):\n        return dict(obj.items())\n\n    # 4. Fallback: Return as is\n    return obj","metadata":{"id":"e5EaHJ7r-LdW","trusted":true,"execution":{"execution_failed":"2026-02-01T12:14:00.453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ACKNOWLEDGEMENT_MESSAGE = \"\"\"\nSubmission of this file and assignment indicate the student's agreement to the following Aknowledgement requirements:\nSetting the ACNKOWLEDGED flag to True indicates full understanding and acceptance of the following:\n1. Slack days may ONLY be used on P2 FINAL (not checkpoint) submission. I.e. you may use slack days to submit final P2 kaggle scores (such as this one) later on the **SLACK KAGGLE COMPETITION** at the expense of your Slack days.\n2. The final autolab **code submission is due 48 hours after** the conclusion of the Kaggle Deadline (or, the same day as your final kaggle submission).\n3. Course staff will require your kaggle username here, and then will pull your official PRIVATE kaggle leaderboard score. This submission may result in slight variance in scores/code, but we will check for acceptable discrepancies. Any discrepancies related to modifying the submission code (at the bottom of the notebook) will result in an AIV.\n4. You are NOT allowed to use any code that will pre-load models (such as those from Hugging Face, etc.).\n   You MAY use models described by papers or articles, but you MUST implement them yourself through fundamental PyTorch operations (i.e. Linear, Conv2d, etc.).\n5. You are NOT allowed to use any external data/datasets at ANY point of this assignment.\n6. You may work with teammates to run ablations/experiments, BUT you must submit your OWN code and your OWN results.\n7. Failure to comply with the prior rules will be considered an Academic Integrity Violation (AIV).\n8. Late submissions MUST be submitted through the Slack Kaggle (see writeup for details). Any submissions made to the regular Kaggle after the original deadline will NOT be considered, no matter how many slack days remain for the student.\n\"\"\"\ndef save_acknowledgment_file():\n    if ACKNOWLEDGED:\n        with open(\"acknowledgement.txt\", \"w\") as f:\n            f.write(ACKNOWLEDGEMENT_MESSAGE.strip())\n        print(\"Saved acknowledgement.txt\")\n        return True\n    else:\n        print(\"ERROR: Must set ACKNOWLEDGED = True.\")\n        return False\n# Saves README\ndef save_readme(readme):\n    try:\n        with open(\"README.txt\", \"w\") as f:\n            f.write(readme.strip())\n\n        print(\"Saved README.txt\")\n    except Exception as e:\n        print(f\"ERROR: Error occured while saving README.txt: {e}\")\n        return False\n\n    return True\n# Saves wandb logs\nimport wandb, json, pickle\n\ndef save_top_wandb_runs():\n    wandb.login(key=WANDB_API_KEY)\n    if not ACKNOWLEDGED:\n        print(\"ERROR: Must set ACKNOWLEDGED = True.\")\n        return False\n\n    api = wandb.Api()\n    runs = api.runs(\n        f\"{WANDB_USERNAME_OR_TEAMNAME}/{WANDB_PROJECT}\",\n        order=f\"{'-' if WANDB_DIRECTION == 'descending' else ''}summary_metrics.{WANDB_METRIC}\"\n    )\n    selected_runs = runs[:min(WANDB_TOP_N, len(runs))]\n\n    if not selected_runs:\n        print(f\"ERROR: No runs found for {WANDB_USERNAME_OR_TEAMNAME}/{WANDB_PROJECT}. Please check that your wandb credentials (Wandb Username/Team Name, API Key, and Project Name) are correct.\")\n        return False\n\n    all_data = []\n    for run in selected_runs:\n        run_data = {\n            \"id\": run.id,\n            \"name\": run.name,\n            \"tags\": run.tags,\n            \"state\": run.state,\n            \"created_at\": str(run.created_at),\n            \"config\": json.loads(json.dumps(strict_wandb_extract(run.config), default=str)),\n            \"summary\": json.loads(json.dumps(strict_wandb_extract(run.summary), default=str))\n        }\n        try:\n            run_data[\"history\"] = run.history(samples=1000)\n        except Exception as e:\n            run_data[\"history\"] = f\"Failed to fetch history: {str(e)}\"\n        all_data.append(run_data)\n    with open(WANDB_OUTPUT_PKL, \"wb\") as f:\n        pickle.dump(all_data, f)\n\n    print(f\"OK: Exported {len(all_data)} WandB runs to {WANDB_OUTPUT_PKL}\")\n\n    return True\n# Saves kaggle information\n\n# Install dependencies silently (only if running on Colab)\nimport sys\n\nfrom datetime import datetime\nimport os, json, requests\ndef kaggle_login(username, key):\n    os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n    with open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"w\") as f:\n        json.dump({\"username\": username, \"key\": key}, f)\n    os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\n\n\ndef get_active_submission_config():\n    if ENABLE_SLACK_SUBMISSION:\n        return SLACK_COMPETITION_NAME, SLACK_SUBMISSION_DATETIME\n    return COMPETITION_NAME, FINAL_SUBMISSION_DATETIME\n\ndef kaggle_user_exists(usernagbme):\n    try:\n        return requests.get(f\"https://www.kaggle.com/{KAGGLE_USERNAME}\").status_code == 200\n    except Exception as e:\n        print(f\"ERROR: Error while checking Kaggle user: {e}\")\n        return False\n\nDEFAULT_SCORE=0\nif GRADING_DIRECTION == \"ascending\":\n    DEFAULT_SCORE=0\nelse:\n    DEFAULT_SCORE=1.0\n\ndef get_best_kaggle_score(subs):\n    def extract_score(s):\n        return float(\n            (getattr(s, \"private_score\", None) or getattr(s, \"privateScore\", None)) or\n            (getattr(s, \"public_score\", None)  or getattr(s, \"publicScore\", None))  or\n            DEFAULT_SCORE\n        )\n    if not subs:\n        return None, None\n    best = max(subs, key=lambda s: extract_score(s) if GRADING_DIRECTION == \"ascending\" else -extract_score(s))\n    score_type = \"private\" if (getattr(best, \"private_score\", None) or getattr(best, \"privateScore\", None)) not in [None, \"\"] else \"public\"\n    return extract_score(best), score_type\n\ndef save_kaggle_json(kaggle_username, kaggle_key):\n\n    kaggle_login(kaggle_username, kaggle_key)\n\n    from kaggle.api.kaggle_api_extended import KaggleApi\n\n    if not ACKNOWLEDGED:\n        print(\"ERROR: Must set ACKNOWLEDGED = True.\")\n        return False\n\n    if not kaggle_user_exists(KAGGLE_USERNAME):\n        print(f\"ERROR: User '{KAGGLE_USERNAME}' not found.\")\n        return False\n\n    comp_name, deadline = get_active_submission_config()\n\n    api = KaggleApi()\n    api.authenticate()\n\n    # Get competition submissions\n    submissions = [s for s in api.competition_submissions(comp_name) if (getattr(s, \"submitted_by\", None) or getattr(s, \"submittedBy\", None)) == KAGGLE_USERNAME and getattr(s, \"date\") <= deadline]\n    if not submissions:\n        print(f\"ERROR: No valid submissions found for user [{KAGGLE_USERNAME}] for this competition [{comp_name}]. Slack flag set to [{ENABLE_SLACK_SUBMISSION}]\")\n        print(\"Please double check your Kaggle username and ensure you've submitted at least once.\")\n        return False\n\n    score, score_type = get_best_kaggle_score(submissions)\n    result = {\n        \"kaggle_username\": KAGGLE_USERNAME,\n        \"acknowledgement\": ACKNOWLEDGED,\n        \"submitted_slack\": ENABLE_SLACK_SUBMISSION,\n        \"competition_name\": comp_name,\n        \"deadline\": deadline.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"raw_score\": score * 100.0,\n        \"score_type\": score_type,\n    }\n\n\n\n    print(f\"OK: Projected score (excluding bonuses) saved as {KAGGLE_OUTPUT_JSON}\")\n    if score:\n        print(f\"Best score {score}.\")\n        with open(KAGGLE_OUTPUT_JSON, \"w\") as f:\n            json.dump(result, f, indent=2)\n        return True\n    return False\n\n\nimport os\nimport sys\nimport zipfile\n\n\ndef create_submission_zip(additional_files, safe_flag):\n    if not \"ACKNOWLEDGED\" in globals() or not ACKNOWLEDGED:\n        print(\"ERROR: Make sure to RUN the Acknowledgement cell (at the top of the notebook). Also, must set ACKNOWLEDGED = True.\")\n        return\n\n    if (not save_acknowledgment_file()):\n        print(\"ERROR: Make sure to RUN the Acknowledgement cell (at the top of the notebook). Also, must set ACKNOWLEDGED = True.\")\n        return\n\n    if not \"ENABLE_SLACK_SUBMISSION\" in globals() or ENABLE_SLACK_SUBMISSION is None:\n        print(\"ERROR: \\\"ENABLE_SLACK_SUBMISSION\\\" variable is not defined. \\nTODO: Make sure to RUN the cell (A few cells up at the beginning of the submission section). \\nMake sure to set the ENABLE_SLACK_SUBMISSION checkbox if you're on colab, or set the parameter correctly set on other platforms \\n(if you are submitting through the SLACK submission).\")\n        return\n\n    if not \"README\" in globals() or not README:\n        print(\"ERROR: Make sure to RUN the README cell(above your credentials cell).\")\n        return\n\n    if (not save_readme(README)):\n        print(\"ERROR: Error while saving the README file. Make sure to complete and RUN the README cell(above your credentials cell).\")\n        return\n\n    if (not save_top_wandb_runs()):\n        return\n\n    if not \"KAGGLE_USERNAME\" in globals() or not \"KAGGLE_API_KEY\" in globals() or not KAGGLE_USERNAME or not KAGGLE_API_KEY:\n        print(\"ERROR: Make sure to set KAGGLE_USERNAME and KAGGLE_API_KEY for this code submission.\")\n        return\n\n    if (not save_kaggle_json(KAGGLE_USERNAME, KAGGLE_API_KEY)):\n        print(f\"ERROR: An error occured while retrieve kaggle information from username [{KAGGLE_USERNAME}] from competition [{get_active_submission_config()[0]}] with slack flag set to [{ENABLE_SLACK_SUBMISSION}]. Please check your kaggle username, key, and submission.\")\n        return\n\n    files_to_zip = [\n        \"acknowledgement.txt\",\n        \"README.txt\",\n        KAGGLE_OUTPUT_JSON,\n        WANDB_OUTPUT_PKL,\n        MODEL_METADATA_JSON,\n        NOTEBOOK_PATH,\n    ] + additional_files\n\n    custom_missing_files_messages = {\n        KAGGLE_OUTPUT_JSON: \"ERROR: Kaggle data retrieval was missing, please check your kaggle username, API Key, and that you have submitted to the correct competition.\"\n    }\n\n    missing_files = False\n\n    with zipfile.ZipFile(SUBMISSION_OUTPUT, \"w\") as zipf:\n        for file_path in files_to_zip:\n            if os.path.exists(file_path):\n                arcname = os.path.basename(file_path)  # flatten path\n                zipf.write(file_path, arcname=arcname)\n                print(f\"OK: Added {arcname}\")\n            else:\n                missing_files = True\n                print(f\"ERROR: Missing file: {file_path}\")\n\n    if missing_files:\n        if safe_flag:\n            print(\"ERROR: Missing files with safety flag set to True. Please upload any necessary files, ensure you have the correct paths and rerun all cells.\")\n            return\n        else:\n            print(\"WARNING: Missing files with safety flag set to False. Submission may be incomplete.\")\n\n    if \"google.colab\" in sys.modules and \"COLAB_GPU\" in os.environ:\n        from google.colab import files\n        files.download(SUBMISSION_OUTPUT)\n\n    print(\"Final submission saved as:\", SUBMISSION_OUTPUT)","metadata":{"id":"Dfc1FzIk7ev7","trusted":true,"execution":{"execution_failed":"2026-02-01T12:14:00.454Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# File Generation (TODO: Check file generation outputs for any errors)","metadata":{"id":"8cnTkp3D7ev8"}},{"cell_type":"code","source":"create_submission_zip(ADDITIONAL_FILES, SAFE_SUBMISSION)\n\n#TODO: If the HW1P2_final_submission.zip file does not\n# automatically bring up a donwload pop-up\n# Then make sure to manually download the HW1P2_final_submission.zip file.","metadata":{"id":"yw-tdFiP7ev8","trusted":true,"execution":{"execution_failed":"2026-02-01T12:14:00.454Z"}},"outputs":[],"execution_count":null}]}